[
["index.html", "CVEN 6833 - Homework 3 Topics", " CVEN 6833 - Homework 3 Alex Belenguer Topics Parametric/Nonparametric Time Series Hidden Markov Model Wavelet Spectral Analysis Extreme Value Time Series Copulas "],
["seasonal-ar1-model.html", "Exercise 1 Seasonal AR(1) model 1.1 Generate 250 simulations each of same length as the historical data. 1.2 Plot statistics from simulations 1.3 Replace the simulation of the errors (or innovations) from Normal to Gamma", " Exercise 1 Seasonal AR(1) model Fit a seasonal AR(1) model – i.e., nonstationary time series model to the monthly Colorado River Flow at Lees Ferry. 1.1 Generate 250 simulations each of same length as the historical data. # Load libraries libr=c(&quot;magrittr&quot;,&quot;sm&quot;,&quot;stats&quot;,&quot;moments&quot;) options(warn=1) suppressPackageStartupMessages(lapply(libr, require, character.only = TRUE)) # import and set up flow data flow = read.table( &quot;http://civil.colorado.edu/~balajir/CVEN6833/HWs/HW-3-2018/LeesFerry-monflows-1906-2016.txt&quot;) flow = flow[,2:13] %&gt;% `rownames&lt;-`(flow[,1]) %&gt;% setNames(.,c(&quot;jan&quot;,&quot;feb&quot;,&quot;mar&quot;,&quot;apr&quot;,&quot;may&quot;,&quot;jun&quot;, &quot;jul&quot;,&quot;aug&quot;,&quot;sep&quot;,&quot;oct&quot;,&quot;nov&quot;,&quot;dec&quot;)) %&gt;% {./10^6} # convert AF to MAF head(flow,n=1L) # show values ## jan feb mar apr may jun jul aug ## 1906 0.244314 0.292534 0.678174 1.20464 3.635101 5.014167 2.95046 1.605086 ## sep oct nov dec ## 1906 1.503159 0.739807 0.503006 0.353312 tail(flow,n=1L) ## jan feb mar apr may jun jul ## 2016 0.360703 0.448837 0.67914 1.099567 2.967581 3.910287 1.342044 ## aug sep oct nov dec ## 2016 0.609946 0.485507 0.546633 0.426289 0.345163 flow$year = rowSums(flow) # add year in 13th column par(mfrow=c(4,3)) # plot histogram and density of monthly flow for(i in 1:12){ hist(flow[,i], freq=FALSE, main = colnames(flow)[i],xlab = &quot;&quot;,ylab = &quot;&quot;) sm.density(flow[,i], add=TRUE) } par(mfrow=c(1,1)) # plot histogram and density of anual flow hist(flow[,13], freq=FALSE, main = colnames(flow)[13],xlab = &quot;&quot;) sm.density(flow[,13], add=TRUE) The seasonal AR model is fitted using the Thomas Fiering coefficients. The anual flow AR model is fitted using stats::arima. # Get the parameters of the Thomas Fiering Model (12 models, 1 for each transition) coef1 = coef2 = rep(0,length.out = 12) coef1[1] = cor(flow[-1,&quot;jan&quot;],flow[-111,&quot;dec&quot;]) # jan - dec coef2[1] = sqrt((var(flow[,1])) * (1. - coef1[1]*coef1[1])) for(i in 2:12){ # remaining month pairs coef1[i] = cor(flow[,i],flow[,i-1]) coef2[i] = sqrt((var(flow[,i])) * (1. - coef1[i]*coef1[i])) } # The anual flow is modeled using single AR(1) model ar.year=ar(flow$year,order.max = 1) #AR order 1, MA The 12 pairs of the TF coefficients are used to run 250 simulations (synthetic values) that will populate the statistics for the models. For the anual flow model, the simulations are obtained via stats::arima.sim. The random gamma values are related to the normal following the relationship explained in the thread below: https://stats.stackexchange.com/questions/37461/the-relationship-between-the-gamma-distribution-and-the-normal-distribution innovation = &quot;Normal&quot; # defines nature of innovations: &quot;Normal&quot; or &quot;Gamma&quot; # parameters for equivalent gamma distribution # N(x;0,s) ~ lim(a -&gt; +inf) G((a-1)*sqrt(1/a)*s;a,sqrt(1/a)*s) a=5 sm=1 #sd for monthly innovations sy=sd(flow$year) #sd for monthly innovations # peak density for each month peak=rep(NA,12) for(i in 1:12){ aux=sm.density(flow[,i],display=&quot;none&quot;) peak[i]=aux$eval.points[which.max(aux$estimate)]*1.5 } # Simulations (innovation defines st. distribution of error) nsim=250 # number of simulations nyrs=length(flow[,1]) # years armean=matrix(0,nsim,12) #matrices that store the statistics arstdev=matrix(0,nsim,12) arcor=matrix(0,nsim,12) arskw=matrix(0,nsim,12) armax=matrix(0,nsim,12) armin=matrix(0,nsim,12) ar.year.stat=matrix(NA,ncol = 6,nrow = nsim) # year statistics colnames(ar.year.stat) = c(&quot;mean&quot;,&quot;stdev&quot;,&quot;min&quot;,&quot;max&quot;,&quot;skew&quot;,&quot;cor&quot;) # Points where May PDF is evaluated xeval=seq(min(flow$may)-0.25*sd(flow$may), max(flow$may)+0.25*sd(flow$may),length=100) simpdf=matrix(0,nrow=nsim,ncol=100) # Array to store May simulated PDF # Points where anual PDF is evaluated yeval=seq(min(flow$year)-0.25*sd(flow$year), max(flow$year)+0.25*sd(flow$year),length=100) year.pdf=matrix(0,nrow=nsim,ncol=100) # Array to store anual simulated PDF for(k in 1:nsim){ nmons=nyrs*12 #number of values to be generated xsim=1:nmons r=sample(1:nyrs,1) xsim[1]=flow[r,1] # Starting point for sim xprev=xsim[1] for(i in 2:nmons){ j=i %% 12 if(j == 0) j=12 j1=j-1 if(j == 1) j1=12 x1=xprev-ifelse(innovation==&quot;Normal&quot;,mean(flow[,j1]),peak[j1]) x2=coef2[j]*ifelse(innovation==&quot;Normal&quot;,rnorm(1,0,1), rgamma(1,shape=a,scale=sqrt(1/a)*sm)-(a-1)*sqrt(1/a)*sm) xsim[i]=mean(flow[,j]) + x1*coef1[j] + x2 xprev=xsim[i] } #Store simulated values in matrix form, get May values and PDF simdismon=matrix(xsim,ncol = 12, byrow = TRUE) # filled by row maysim = simdismon[,5] # Synthetic values for May simpdf[k,]=sm.density(maysim,eval.points=xeval,display=&quot;none&quot;)$estimate # Fill statistics for each month for(j in 1:12){ armean[k,j]=mean(simdismon[,j]) armax[k,j]=max(simdismon[,j]) armin[k,j]=min(simdismon[,j]) arstdev[k,j]=sd(simdismon[,j]) arskw[k,j]=skewness(simdismon[,j]) } arcor[k,1]=cor(simdismon[-nyrs,12],simdismon[2:nyrs,1]) #cor dec-jan for(j in 2:12){ # rest of pairs j1=j-1 arcor[k,j]=cor(simdismon[,j],simdismon[,j1]) } # anual flow simulations if(innovation==&quot;Normal&quot;){ ar.year.sim = arima.sim(n = nyrs, list(ar = ar.year$ar), sd = sqrt(ar.year$var.pred)) + mean(flow$year) }else{ ar.year.sim = arima.sim(n = nyrs, list(ar = ar.year$ar), rand.gen = function(n, ...) rgamma(n,shape=a, scale=sqrt(1/a)*sy)-(a-1)*sqrt(1/a)*sy) + yeval[which.max(year.density)]*0.85 } # Get anual PDF year.pdf[k,]=sm.density(ar.year.sim,eval.points= yeval,display=&quot;none&quot;)$estimate # Calculate statistics ar.year.stat[k,&quot;mean&quot;]=mean(ar.year.sim) ar.year.stat[k,&quot;max&quot;]=max(ar.year.sim) ar.year.stat[k,&quot;min&quot;]=min(ar.year.sim) ar.year.stat[k,&quot;stdev&quot;]=sd(ar.year.sim) ar.year.stat[k,&quot;skew&quot;]=skewness(ar.year.sim) ar.year.stat[k,&quot;cor&quot;]=cor(ar.year.sim[-nyrs],ar.year.sim[2:nyrs]) } The statistics from the synthetic values and the historical data are bound in the same matrix. # Compute statistics from the historical data. obsmean=1:12 obsstdev=1:12 obscor=1:12 obsskw=1:12 obsmax=1:12 obsmin=1:12 for(i in 1:12){ obsmax[i]=max(flow[,i]) obsmin[i]=min(flow[,i]) obsmean[i]=mean(flow[,i]) obsstdev[i]=sd(flow[,i]) obsskw[i]=skewness(flow[,i]) } obscor[1]= cor(flow[-nyrs,12], flow[2:nyrs,1]) for(i in 2:12){ i1=i-1 obscor[i]=cor(flow[,i], flow[,i1]) } # bind the stats of the historic data at the top.. armean=rbind(obsmean,armean) arstdev=rbind(obsstdev,arstdev) arskw=rbind(obsskw,arskw) arcor=rbind(obscor,arcor) armax=rbind(obsmax,armax) armin=rbind(obsmin,armin) # anual flow binding year.stat=c(mean(flow$year),sd(flow$year),min(flow$year), max(flow$year),skewness(flow$year), cor(flow$year[-nyrs],flow$year[2:nyrs])) ar.year.stat = rbind(year.stat,ar.year.stat) 1.2 Plot statistics from simulations Create boxplots of annual and monthly, mean, variance, skew, lag-1 correlation, minimum, maximum and PDFs of May and annual flows. Comment on what you observe and also on why some of the monthly statistics are not captured. # function to plot boxplots with the structure: hist. in first row plot.bp = function(matrix,name){ xmeans=as.matrix(matrix) n=length(xmeans[,1]) xmeans1=as.matrix(xmeans[2:n,]) #the first row is the original data xs=1:12 zz=boxplot(split(xmeans1,col(xmeans1)), plot=F, cex=1.0) zz$names=rep(&quot;&quot;,length(zz$names)) z1=bxp(zz,ylim=range(xmeans),xlab=&quot;&quot;,ylab=&quot;&quot;,cex=1.00) points(z1,xmeans[1,],pch=16, col=&quot;red&quot;) lines(z1,xmeans[1,],pch=16, col=&quot;gray&quot;) title(main=name) } The plots for the statistics of the simulated time series (shown as boxplots) vs. the historical data (shown as points and lines) are reproduced below: par(mfrow=c(3,2)) plot.bp(armean,&quot;Mean&quot;) plot.bp(arstdev,&quot;Standard Deviation&quot;) plot.bp(armin,&quot;Min&quot;) plot.bp(armax,&quot;Max&quot;) plot.bp(arskw,&quot;Skews&quot;) plot.bp(arcor,&quot;Lag-1 correlation&quot;) The model proficiently captures the mean and max values. A fair fit is obtained with the standard deviation. However, the normality of the innovations results in a poor fit of minimum values and skews. The anual statistics are similarly represented below: par(mfrow=c(2,3)) plot.bp(ar.year.stat[,&quot;mean&quot;],&quot;Mean&quot;) plot.bp(ar.year.stat[,&quot;stdev&quot;],&quot;Standard Deviation&quot;) plot.bp(ar.year.stat[,&quot;min&quot;],&quot;Min&quot;) plot.bp(ar.year.stat[,&quot;max&quot;],&quot;Max&quot;) plot.bp(ar.year.stat[,&quot;skew&quot;],&quot;Skews&quot;) plot.bp(ar.year.stat[,&quot;cor&quot;],&quot;Lag-1 correlation&quot;) The best fitting occurs for mean, sd, and correlation. Min, max and skew hardly contain historical values within the 25th/75th percentile limits. The simulated May PDF vs. the historical May PDF is plotted at 100 points. xdensityorig = flow$may %&gt;% sm.density(.,eval.points=xeval,display=&quot;none&quot;) %&gt;% .$estimate plot.pdf = function(eval,histPDF,simPDF){ xeval = eval plot(xeval,histPDF,pch=&quot;.&quot;,col=&quot;red&quot;,ylim=range(simPDF,histPDF), xlab=&quot;&quot;,ylab = &quot;&quot;) for(i in 1:nsim)lines(xeval,simPDF[i,],col=&#39;lightgrey&#39;,lty=3) lines(xeval,histPDF,lwd=3,col=&quot;red&quot;) title(main=&quot;Historical vs. simulated PDF&quot;) } plot.pdf(xeval,xdensityorig,simpdf) The bimodal historical May PDF is not captured by the simulations due to the Normal nature of the innovations. The simulated vs. historical anual flow PDF is similarly compared. year.density = flow$year %&gt;% sm.density(.,eval.points=yeval, display=&quot;none&quot;) %&gt;% .$estimate plot.pdf(yeval,year.density,year.pdf) 1.3 Replace the simulation of the errors (or innovations) from Normal to Gamma The simulation code chunks are rerun via r markdown code with innovation = “Gamma”. innovation=&quot;Gamma&quot; par(mfrow=c(3,2)) plot.bp(armean,&quot;Mean&quot;) plot.bp(arstdev,&quot;Standard Deviation&quot;) plot.bp(armin,&quot;Min&quot;) plot.bp(armax,&quot;Max&quot;) plot.bp(arskw,&quot;Skews&quot;) plot.bp(arcor,&quot;Lag-1 correlation&quot;) The graphs show a differentiated skew performance, although the fit seems to be equivalent. par(mfrow=c(2,3)) plot.bp(ar.year.stat[,&quot;mean&quot;],&quot;Mean&quot;) plot.bp(ar.year.stat[,&quot;stdev&quot;],&quot;Standard Deviation&quot;) plot.bp(ar.year.stat[,&quot;min&quot;],&quot;Min&quot;) plot.bp(ar.year.stat[,&quot;max&quot;],&quot;Max&quot;) plot.bp(ar.year.stat[,&quot;skew&quot;],&quot;Skews&quot;) plot.bp(ar.year.stat[,&quot;cor&quot;],&quot;Lag-1 correlation&quot;) plot.pdf(xeval,xdensityorig,simpdf) The simulated May PDF is no longer symmetric, as it would be expected from a Gamma Distribution plot.pdf(yeval,year.density,year.pdf) The same effect is depicted in the anual PDF. "],
["nonparametric-seasonal-lag-1-model.html", "Exercise 2 Nonparametric seasonal lag-1 model 2.1 Generate 250 simulations each of same length as the historical data. 2.2 Plot statistics from simulations 2.3 Advantages/disadvantages with a nonparametric approach.", " Exercise 2 Nonparametric seasonal lag-1 model Fit a nonparametric seasonal lag-1 model and repeat [exercise] 1. You can use either the the K-nn bootstrap technique or LOCFIT/residual resampling and repeat 1. 2.1 Generate 250 simulations each of same length as the historical data. # Load libraries libr=c(&quot;magrittr&quot;,&quot;sm&quot;,&quot;stats&quot;,&quot;moments&quot;) options(warn=1) suppressPackageStartupMessages(lapply(libr, require, character.only = TRUE)) # import and set up flow data flow = read.table( &quot;http://civil.colorado.edu/~balajir/CVEN6833/HWs/HW-3-2018/LeesFerry-monflows-1906-2016.txt&quot;) flow = flow[,2:13] %&gt;% `rownames&lt;-`(flow[,1]) %&gt;% setNames(.,c(&quot;jan&quot;,&quot;feb&quot;,&quot;mar&quot;,&quot;apr&quot;,&quot;may&quot;,&quot;jun&quot;, &quot;jul&quot;,&quot;aug&quot;,&quot;sep&quot;,&quot;oct&quot;,&quot;nov&quot;,&quot;dec&quot;)) %&gt;% {./10^6} # convert AF to MAF flow$year = rowSums(flow) # add year in 13th column head(flow,n=1L) # show values ## jan feb mar apr may jun jul aug ## 1906 0.244314 0.292534 0.678174 1.20464 3.635101 5.014167 2.95046 1.605086 ## sep oct nov dec year ## 1906 1.503159 0.739807 0.503006 0.353312 18.72376 tail(flow,n=1L) ## jan feb mar apr may jun jul ## 2016 0.360703 0.448837 0.67914 1.099567 2.967581 3.910287 1.342044 ## aug sep oct nov dec year ## 2016 0.609946 0.485507 0.546633 0.426289 0.345163 13.2217 # Simulations (innovation defines st. distribution of error) nsim=100 # number of simulations nsim1=nsim+1 nyrs=length(flow[,1]) # years nyrs1 = nyrs-1 N=nyrs*12 # time series for monthly flow armean=matrix(0,nsim,12) #matrices that store the statistics arstdev=matrix(0,nsim,12) arcor=matrix(0,nsim,12) arskw=matrix(0,nsim,12) armax=matrix(0,nsim,12) armin=matrix(0,nsim,12) ar.year.stat=matrix(NA,ncol = 6,nrow = nsim) # year statistics colnames(ar.year.stat) = c(&quot;mean&quot;,&quot;stdev&quot;,&quot;min&quot;,&quot;max&quot;,&quot;skew&quot;,&quot;cor&quot;) # Points where May PDF is evaluated xeval=seq(min(flow$may)-0.25*sd(flow$may), max(flow$may)+0.25*sd(flow$may),length=100) simpdf=matrix(0,nrow=nsim,ncol=100) # Array to store May simulated PDF # Points where anual PDF is evaluated yeval=seq(min(flow$year)-0.25*sd(flow$year), max(flow$year)+0.25*sd(flow$year),length=100) year.pdf=matrix(0,nrow=nsim,ncol=100) # Array to store anual simulated PDF # # The anual flow is modeled using single AR(1) model # ar.year=ar(flow$year,order.max = 1) #AR order 1, MA # Fitting Np-AR-1 model K=round(sqrt(nyrs)) #number of nearest neighbors W= 1:K %&gt;% {1/.} %&gt;% {./sum(.)} %&gt;% cumsum(.) # weight function for (isim in 1:nsim) { i=round(runif(1,2,nyrs)) # initial random year zsim = 1:N # n of monthly flow time series zsim[1] = flow[i,1] # initial flow (Jan of year i) xp = zsim[1] # Previous value initialization # Monthly flow simulations for (j in 2:N) { mo = j%%12 # month we are simulating imon=ifelse(mo==0,12,mo) if (mo != 1){ # if mo = 0 (Dec) data = flow[,1:12] # data is the same as test (Jan - Dec) } else { # Jan data = cbind(flow[2:nyrs,1],flow[1:nyrs1,2:12]) } # if mo = 1 (JAn), 1st col of data will be March, last col will be Feb mo1 = mo-1 if(mo == 0)mo1=11 if(mo == 1)mo1=12 xdist = order(abs(xp - data[,mo1])) # if (mo == 1) { # xdist = order(abs(xp - data[,mo1]))} # if (mo &gt;1) { # xx=rbind(xp, data[,(12-ilag):11]) # xdist = order(as.matrix(dist(xx))[1,2:nyrs+1]) } xx=runif(1,0,1) # generate random number xy=c(xx,W) xx=rank(xy) # get rank with respect to W i1=xdist[xx[1]] # distance order at given rank zsim[j]=data[i1,imon] # select data at given distance xp = zsim[j] # store previous year for next it. } # end j loop simdismon = matrix(zsim, ncol=12, byrow = TRUE) # makes a 12 column matrix with jan thru dec maysim = simdismon[,5] simpdf[isim,]=sm.density(maysim,eval.points=xeval,display=&quot;none&quot;)$estimate # Fill statistics for each month for(j in 1:12){ armean[isim,j]=mean(simdismon[,j]) armax[isim,j]=max(simdismon[,j]) armin[isim,j]=min(simdismon[,j]) arstdev[isim,j]=sd(simdismon[,j]) arskw[isim,j]=skewness(simdismon[,j]) } arcor[isim,1]=cor(simdismon[-nyrs,12],simdismon[2:nyrs,1]) #cor dec-jan for(j in 2:12){ # rest of pairs j1=j-1 arcor[isim,j]=cor(simdismon[,j],simdismon[,j1]) } # Anual flow simulations y.sim = 1:nyrs # n of anual flow time series y.sim[1] = flow[i,&quot;year&quot;] # initial flow (Year i) y.p = y.sim[1] # Previous value initialization y.data=flow$year # dataset is fixed for anual ts. for(j in 2:nyrs){ y.dist = order(abs(y.p - y.data)) i1 = runif(1,0,1) %&gt;% c(.,W) %&gt;% rank(.) %&gt;% {y.dist[.[1]]} y.sim[j]=y.data[i1] # select y.data at given distance y.p = y.sim[j] # store previous year for next it. } # Get anual PDF year.pdf[isim,]=sm.density(y.sim,eval.points= yeval,display=&quot;none&quot;)$estimate # Calculate statistics ar.year.stat[isim,&quot;mean&quot;]=mean(y.sim) ar.year.stat[isim,&quot;max&quot;]=max(y.sim) ar.year.stat[isim,&quot;min&quot;]=min(y.sim) ar.year.stat[isim,&quot;stdev&quot;]=sd(y.sim) ar.year.stat[isim,&quot;skew&quot;]=skewness(y.sim) ar.year.stat[isim,&quot;cor&quot;]=cor(y.sim[-nyrs],y.sim[2:nyrs]) } # end isim loop We add the statistics from historical data in the first row of the tables # Compute statistics from the historical data. obsmean=1:12 obsstdev=1:12 obscor=1:12 obsskw=1:12 obsmax=1:12 obsmin=1:12 for(i in 1:12){ obsmax[i]=max(flow[,i]) obsmin[i]=min(flow[,i]) obsmean[i]=mean(flow[,i]) obsstdev[i]=sd(flow[,i]) obsskw[i]=skewness(flow[,i]) } obscor[1]= cor(flow[-nyrs,12], flow[2:nyrs,1]) for(i in 2:12){ i1=i-1 obscor[i]=cor(flow[,i], flow[,i1]) } # bind the stats of the historic data at the top.. armean=rbind(obsmean,armean) arstdev=rbind(obsstdev,arstdev) arskw=rbind(obsskw,arskw) arcor=rbind(obscor,arcor) armax=rbind(obsmax,armax) armin=rbind(obsmin,armin) # anual flow binding year.stat=c(mean(flow$year),sd(flow$year),min(flow$year), max(flow$year),skewness(flow$year), cor(flow$year[-nyrs],flow$year[2:nyrs])) ar.year.stat = rbind(year.stat,ar.year.stat) 2.2 Plot statistics from simulations Create boxplots of annual and monthly, mean, variance, skew, lag-1 correlation, minimum, maximum and PDFs of May and annual flows. Comment on what you observe and also on why some of the monthly statistics are not captured. # function to plot boxplots with the structure: hist. in first row plot.bp = function(matrix,name){ xmeans=as.matrix(matrix) n=length(xmeans[,1]) xmeans1=as.matrix(xmeans[2:n,]) #the first row is the original data xs=1:12 zz=boxplot(split(xmeans1,col(xmeans1)), plot=F, cex=1.0) zz$names=rep(&quot;&quot;,length(zz$names)) z1=bxp(zz,ylim=range(xmeans),xlab=&quot;&quot;,ylab=&quot;&quot;,cex=1.00) points(z1,xmeans[1,],pch=16, col=&quot;red&quot;) lines(z1,xmeans[1,],pch=16, col=&quot;gray&quot;) title(main=name) } The plots for the statistics of the simulated time series (shown as boxplots) vs. the historical data (shown as points and lines) are reproduced below: par(mfrow=c(3,2)) plot.bp(armean,&quot;Mean&quot;) plot.bp(arstdev,&quot;Standard Deviation&quot;) plot.bp(armin,&quot;Min&quot;) plot.bp(armax,&quot;Max&quot;) plot.bp(arskw,&quot;Skews&quot;) plot.bp(arcor,&quot;Lag-1 correlation&quot;) All the statistics are well fitted with respect the historical data. The anual statistics are similarly represented below: par(mfrow=c(2,3)) plot.bp(ar.year.stat[,&quot;mean&quot;],&quot;Mean&quot;) plot.bp(ar.year.stat[,&quot;stdev&quot;],&quot;Standard Deviation&quot;) plot.bp(ar.year.stat[,&quot;min&quot;],&quot;Min&quot;) plot.bp(ar.year.stat[,&quot;max&quot;],&quot;Max&quot;) plot.bp(ar.year.stat[,&quot;skew&quot;],&quot;Skews&quot;) plot.bp(ar.year.stat[,&quot;cor&quot;],&quot;Lag-1 correlation&quot;) For the anual flow, this is no longer true. Some statistics are not poorly fitted. xdensityorig = flow$may %&gt;% sm.density(.,eval.points=xeval,display=&quot;none&quot;) %&gt;% .$estimate plot.pdf = function(eval,histPDF,simPDF){ xeval = eval plot(xeval,histPDF,pch=&quot;.&quot;,col=&quot;red&quot;,ylim=range(simPDF,histPDF), xlab=&quot;&quot;,ylab = &quot;&quot;) for(i in 1:nsim)lines(xeval,simPDF[i,],col=&#39;lightgrey&#39;,lty=3) lines(xeval,histPDF,lwd=3,col=&quot;red&quot;) title(main=&quot;Historical vs. simulated PDF&quot;) } plot.pdf(xeval,xdensityorig,simpdf) The simulated PDF shows the bimodal behavior, and does not respond to a probabilistic distribution, as it happened to parametric AR. year.density = flow$year %&gt;% sm.density(.,eval.points=yeval, display=&quot;none&quot;) %&gt;% .$estimate plot.pdf(yeval,year.density,year.pdf) Many results are repeated (only 111 values for 100 simulations of 111 time series), creating a crisp PDF. Not realistic. 2.3 Advantages/disadvantages with a nonparametric approach. When the dataset is large enough, the fitted PDF and statistics over perform the parametric modelling. When the opposite occurs, many values are repeatedly picked, resulting in biased PDF and statistics. "],
["multivariate-simulation-copulas.html", "Exercise 3 Multivariate Simulation - Copulas 3.1 Fit a Copula to the joint CDF (copula by pairs) 3.2 Simulate from the Copula and invert 3.3 Boxplot the statistics listed in problem 1 and compare with the results from the previous methods 3.4 12 months copula", " Exercise 3 Multivariate Simulation - Copulas Another approach to simulating the monthly streamflow is using Copulas. Fit appropriate marginal PDFs for each monthly streamflow 3.0.1 Data import and setup # Load libraries libr=c(&quot;magrittr&quot;,&quot;sm&quot;,&quot;MASS&quot;,&quot;ks&quot;,&quot;copula&quot;,&quot;moments&quot;) options(warn=1) suppressPackageStartupMessages(lapply(libr, require, character.only = TRUE)) # import and set up flow data flow = read.table( &quot;http://civil.colorado.edu/~balajir/CVEN6833/HWs/HW-3-2018/LeesFerry-monflows-1906-2016.txt&quot;) flow = flow[,2:13] %&gt;% `rownames&lt;-`(flow[,1]) %&gt;% setNames(.,c(&quot;jan&quot;,&quot;feb&quot;,&quot;mar&quot;,&quot;apr&quot;,&quot;may&quot;,&quot;jun&quot;, &quot;jul&quot;,&quot;aug&quot;,&quot;sep&quot;,&quot;oct&quot;,&quot;nov&quot;,&quot;dec&quot;)) %&gt;% {./10^6} # convert AF to MAF head(flow,n=1L) # show values ## jan feb mar apr may jun jul aug ## 1906 0.244314 0.292534 0.678174 1.20464 3.635101 5.014167 2.95046 1.605086 ## sep oct nov dec ## 1906 1.503159 0.739807 0.503006 0.353312 tail(flow,n=1L) ## jan feb mar apr may jun jul ## 2016 0.360703 0.448837 0.67914 1.099567 2.967581 3.910287 1.342044 ## aug sep oct nov dec ## 2016 0.609946 0.485507 0.546633 0.426289 0.345163 3.1 Fit a Copula to the joint CDF (copula by pairs) As a first approach, we will use copula by pairwise comparison (lag-1 model). # Function to fit copula estimator for a pair of time series fit2copula = function(data1,data2){ u = pobs(cbind(data1,data2)) fc=normalCopula(dim=2,disp=&#39;un&#39;) fnc = fitCopula(fc,u) return(fnc) } # Function to generate simulated values from fitted copula sim.copula = function(fnc,data2,N){ cop.sim = rCopula(N,normalCopula(fnc,dim=2,dispstr=&#39;un&#39;)) data.sim = quantile(data2,cop.sim[,2]) # quan.sim = qkde(cop.sim[,2],fhat) return(data.sim) # return(quan.sim) } 3.2 Simulate from the Copula and invert # Simulation paramenters nsim=250 N=length(flow[,1]) # years armean=matrix(0,nsim,12) #matrices to store the statistics arstdev=matrix(0,nsim,12) arcor=matrix(0,nsim,12) arskw=matrix(0,nsim,12) armax=matrix(0,nsim,12) armin=matrix(0,nsim,12) xeval = matrix(0,nrow = 100,ncol = 12) # Points where the PDF is evaluated for(i in 1:12){ xeval[,i]=seq(min(flow[,i])-0.25*sd(flow[,i]), max(flow[,i])+0.25*sd(flow[,i]),length=100) } simpdf = array(0,c(100,12,nsim)) # Array to store simulated PDF xsim = array(0,c(N,12,nsim)) # Array to store simulated val # Simulation initialization parameters ncop = 12 # generate pairs of data to use in copula data1 = data2 = array(NA,c(N,1,ncop)) # data 1 / 2: prev / current month fnc = 1:ncop # fitted normal copulas for every pair for(icop in 1:ncop){ if(icop==1){ data1[-N,1,icop] = flow$dec[-N] # dec of previous year data2[-N,1,icop] = flow$jan[2:N] # jan of next year fnc[icop] = fit2copula(data1[-N,,icop],data2[-N,,icop])@estimate }else{ data1[,1,icop] = flow[,icop-1] # as given data2[,1,icop] = flow[,icop] # as given fnc[icop] = fit2copula(data1[,,icop],data2[,,icop])@estimate } } for(isim in 1:nsim){ # simulation loop for(j in 1:12){ # get each month as copula with previous month if(j==1){ # dec-jan copula xsim[,j,isim]=sim.copula(fnc[j],data2[-N,,j],N) # remove NA year 111 }else{ xsim[,j,isim]=sim.copula(fnc[j],data2[,,j],N) # simulated values } simpdf[,j,isim]=sm.density(xsim[,j,isim], # simulated PDF eval.points=xeval[,j],display=&quot;none&quot;)$estimate # fill statistics armean[isim,j]=mean(xsim[,j,isim]) armax[isim,j]=max(xsim[,j,isim]) armin[isim,j]=min(xsim[,j,isim]) arstdev[isim,j]=sd(xsim[,j,isim]) arskw[isim,j]=skewness(xsim[,j,isim]) if(j&gt;2)arcor[isim,j]=cor(xsim[,j,isim],xsim[,j-1,isim]) } arcor[isim,1]=cor(xsim[-N,12,isim],xsim[2:N,1,isim]) # print(isim) } We now add the statistics from the original dataset and bind it with the simulation st. # Compute statistics from the historical data. obsmean=1:12 obsstdev=1:12 obscor=1:12 obsskw=1:12 obsmax=1:12 obsmin=1:12 for(i in 1:12){ obsmax[i]=max(flow[,i]) obsmin[i]=min(flow[,i]) obsmean[i]=mean(flow[,i]) obsstdev[i]=sd(flow[,i]) obsskw[i]=skewness(flow[,i]) } obscor[1]= cor(flow[-N,12], flow[2:N,1]) for(i in 2:12){ obscor[i]=cor(flow[,i], flow[,i-1]) } # bind the stats of the historic data at the top.. armean=rbind(obsmean,armean) arstdev=rbind(obsstdev,arstdev) arskw=rbind(obsskw,arskw) arcor=rbind(obscor,arcor) armax=rbind(obsmax,armax) armin=rbind(obsmin,armin) 3.3 Boxplot the statistics listed in problem 1 and compare with the results from the previous methods We repeat the code from problem 1 to do so. # function to plot boxplots with the structure: hist. in first row plot.bp = function(matrix,name){ xmeans=as.matrix(matrix) n=length(xmeans[,1]) xmeans1=as.matrix(xmeans[2:n,]) #the first row is the original data xs=1:12 zz=boxplot(split(xmeans1,col(xmeans1)), plot=F, cex=1.0) zz$names=rep(&quot;&quot;,length(zz$names)) z1=bxp(zz,ylim=range(xmeans),xlab=&quot;&quot;,ylab=&quot;&quot;,cex=1.00) points(z1,xmeans[1,],pch=16, col=&quot;red&quot;) lines(z1,xmeans[1,],pch=16, col=&quot;gray&quot;) title(main=name) } par(mfrow=c(3,2)) plot.bp(armean,&quot;Mean&quot;) plot.bp(arstdev,&quot;Standard Deviation&quot;) plot.bp(armin,&quot;Min&quot;) plot.bp(armax,&quot;Max&quot;) plot.bp(arskw,&quot;Skews&quot;) plot.bp(arcor,&quot;Lag-1 correlation&quot;) As observed, mean, st.dev, max and skews are well fit. Min values are overestimated, and lag-1 is not captured. In general, the statics are better than the ones using autoregressive models. xdensityorig = matrix (0,nrow = 100,ncol = 12) #initialize original PDF for(j in 1:12){ # obtain original PDF xdensityorig[,j] = flow[,j] %&gt;% sm.density(.,eval.points=xeval[,j],display=&quot;none&quot;) %&gt;% .$estimate } plot.pdf = function(eval,histPDF,simPDF,title){ xeval = eval plot(xeval,histPDF,pch=&quot;.&quot;,col=&quot;red&quot;,ylim=range(simPDF,histPDF), xlab=&quot;&quot;,ylab = &quot;&quot;) for(i in 1:nsim)lines(xeval,simPDF[,i],col=&#39;lightgrey&#39;,lty=3) lines(xeval,histPDF,lwd=3,col=&quot;red&quot;) title(main=title) } par(mfrow=c(4,3)) par(mar=c(2,2,2,2)) for(j in 1:12){ plot.pdf(xeval[,j],xdensityorig[,j],simpdf[,j,],colnames(flow[j])) } The simulated PDF do not respond to a probabilistic distribution, sitting closely around the empirical PDF. The result is more precise that the one using autoregressive models. 3.4 12 months copula Now we will use the full copula between all months. u = pobs(flow) # convert to pseudo-observations fc12=normalCopula(dim=12,disp=&#39;un&#39;) # normal copula (66) fnc12 = fitCopula(fc12,u) # estimate from copula dat_sim = array(0,c(N,12,nsim)) cop_pdf = array(0,c(100,12,nsim)) carmean=matrix(0,nsim,12) #matrices to store the statistics carstdev=matrix(0,nsim,12) carcor=matrix(0,nsim,12) carskw=matrix(0,nsim,12) carmax=matrix(0,nsim,12) carmin=matrix(0,nsim,12) for(isim in 1:nsim){ cop_sim = rCopula(N,normalCopula(fnc12@estimate,dim=12,dispstr=&#39;un&#39;)) for(j in 1:12){ dat_sim[,j,isim] = quantile(flow[,j],cop_sim[,j]) cop_pdf[,j,isim]=sm.density(dat_sim[,j,isim], # simulated PDF eval.points=xeval[,j],display=&quot;none&quot;)$estimate # fill statistics carmean[isim,j]=mean(dat_sim[,j,isim]) carmax[isim,j]=max(dat_sim[,j,isim]) carmin[isim,j]=min(dat_sim[,j,isim]) carstdev[isim,j]=sd(dat_sim[,j,isim]) carskw[isim,j]=skewness(dat_sim[,j,isim]) if(j&gt;2)carcor[isim,j]=cor(dat_sim[,j,isim],dat_sim[,j-1,isim]) } carcor[isim,1]=cor(dat_sim[-N,12,isim],dat_sim[2:N,1,isim]) # print(isim) } # bind the stats of the historic data at the top.. carmean=rbind(obsmean,carmean) carstdev=rbind(obsstdev,carstdev) carskw=rbind(obsskw,carskw) carcor=rbind(obscor,carcor) carmax=rbind(obsmax,carmax) carmin=rbind(obsmin,carmin) par(mfrow=c(3,2)) plot.bp(carmean,&quot;Mean&quot;) plot.bp(carstdev,&quot;Standard Deviation&quot;) plot.bp(carmin,&quot;Min&quot;) plot.bp(carmax,&quot;Max&quot;) plot.bp(carskw,&quot;Skews&quot;) plot.bp(carcor,&quot;Lag-1 correlation&quot;) The results are at least as good as the copula by pairs. Also, the lag-1 correlation is captured (with the exception of jan and feb). par(mfrow=c(4,3)) par(mar=c(2,2,2,2)) for(j in 1:12){ plot.pdf(xeval[,j],xdensityorig[,j],cop_pdf[,j,],colnames(flow[j])) } The simulated PDF are also really close to the empirical PDF, being able to mimic the multimodal behavior of February, May, July, etc. "],
["non-stationary-time-series-hidden-markov-model.html", "Exercise 4 Non stationary time series - Hidden Markov Model 4.1 Fit a best HMM for the May Lees Ferry streamflow 4.2 Generate 250 simulations from the fitted HMM 4.3 Boxplot the resulting statistics 4.4 Fit a GLM for the state series 4.5 Use the state GLM to simulate flows from the component distribution", " Exercise 4 Non stationary time series - Hidden Markov Model Another way to simulate a time series is using Hidden Markov Model (Markov Chain + resampling). First, we load the libraries, streamflow data, and functions via R markdown (attached at the end of the document). # libraries libr=c(&quot;HiddenMarkov&quot;,&quot;ggplot2&quot;,&quot;data.table&quot;,&quot;ggthemes&quot;, &quot;magrittr&quot;,&quot;sm&quot;,&quot;moments&quot;,&quot;MASS&quot;,&quot;leaps&quot;) options(warn=1) suppressPackageStartupMessages(lapply(libr, require, character.only = TRUE)) ## Load flow data flow = read.table( &quot;http://civil.colorado.edu/~balajir/CVEN6833/HWs/HW-3-2018/LeesFerry-monflows-1906-2016.txt&quot;) flow = flow[,2:13] %&gt;% `rownames&lt;-`(flow[,1]) %&gt;% setNames(.,c(&quot;jan&quot;,&quot;feb&quot;,&quot;mar&quot;,&quot;apr&quot;,&quot;may&quot;,&quot;jun&quot;, &quot;jul&quot;,&quot;aug&quot;,&quot;sep&quot;,&quot;oct&quot;,&quot;nov&quot;,&quot;dec&quot;)) %&gt;% {./10^6} # convert AF to MAF x = flow$may ## select the May month flows 4.1 Fit a best HMM for the May Lees Ferry streamflow The code below fits HMM models of orders 2 through 6 and calculates the AIC for each. The best order is the one with the least value of AIC. ## Fit HMM models of orders 2 through 6. Obtain the AIC for each ## Best order is the one with the least value of AIC. family &lt;- &quot;gamma&quot; # underlying distribution for hmm discrete &lt;- FALSE aic1=c() for(imodel in 2:6){ m &lt;- imodel #model order to fit stationary &lt;- F # use a stationary distribution of mixtures # different initial condition types when family == &quot;norm&quot; ic &lt;- &quot;same.sd&quot;#c(&quot;same.sd&quot;,&quot;same.both&quot;,&quot;both.diff&quot;) fd.name &lt;- ifelse(family == &quot;norm&quot;, &quot;normal&quot;, family) Pi &lt;- Pi_init(m) # T.P.M. delta &lt;- delta_init(m) pars &lt;- get.named.parlist(x, m, fd.name, lower=.0, ic)#,start=list(shape1=2,shape2=2)) # set up the model hmm &lt;- dthmm(x, Pi=Pi, delta=delta, family, pars, nonstat=!stationary, discrete = discrete) sink(&quot;p.4.hmm.fit&quot;) if(imodel &lt; 2){ hmm &lt;- BaumWelch(hmm, bwcontrol(maxiter = 1000, posdiff=TRUE,converge = expression(diff &gt; tol))) } else { hmm &lt;- BaumWelch(hmm, bwcontrol(maxiter = 1000, tol = 1e-08)) } sink() # get the hidden states from the fitted model # Global decoding. To get the probability of being in a state: hmm$u decoding &lt;- Viterbi(hmm) # get AIC aic &lt;- AIC(hmm) aic1=c(aic1,aic) } We select the HMM with the lowest AIC. In this case, this happens to be of order 2. We rerun the HMM for best order (m=2) and generate the state sequence (decoding) resulting from it. The model summary is also attached. ## Get the best order bestorder = order(aic1)[1] +1 ## Fit the model for this best order m &lt;- bestorder #model order to fit stationary &lt;- F # use a stationary distribution of mixtures # different initial condition types when family == &quot;norm&quot; ic &lt;- &quot;same.sd&quot;#c(&quot;same.sd&quot;,&quot;same.both&quot;,&quot;both.diff&quot;) fd.name &lt;- ifelse(family == &quot;norm&quot;, &quot;normal&quot;, family) Pi &lt;- Pi_init(m) # T.P.M. delta &lt;- delta_init(m) pars &lt;- get.named.parlist(x, m, fd.name, lower=.0, ic)#,start=list(shape1=2,shape2=2)) # set up the model hmm &lt;- dthmm(x, Pi=Pi, delta=delta, family, pars, nonstat=!stationary, discrete = discrete) sink(&quot;p.4.best.hmm&quot;) hmm &lt;- BaumWelch(hmm, bwcontrol(maxiter = 1000, tol = 1e-08)) sink() # end hidding output decoding &lt;- Viterbi(hmm) print(summary(hmm)) ## $delta ## [1] 3.834696e-105 1.000000e+00 ## ## $Pi ## [,1] [,2] ## [1,] 0.6585766 0.3414234 ## [2,] 0.2953841 0.7046159 ## ## $nonstat ## [1] TRUE ## ## $distn ## [1] &quot;gamma&quot; ## ## $pm ## $pm$rate ## [1] 2.974813 4.062195 ## ## $pm$shape ## [1] 6.992011 15.203081 ## ## ## $discrete ## [1] FALSE ## ## $n ## [1] 111 cat(&#39;Model order:&#39;,m,&#39;\\n&#39;) ## Model order: 2 p &lt;- ggplot_stationary_hmm(hmm,.5) print(p) ## Warning: Removed 1 rows containing missing values (geom_bar). state.1=ifelse(decoding==1,2.5,NA) state.2=ifelse(decoding==2,0.5,NA) plot(flow$may-mean(flow$may)+1.5,type=&quot;l&quot;,ylab=&quot;&quot;, main=&quot;Modified May flows and states (supperposed)&quot;) points(state.2,col=&#39;blue&#39;) points(state.1,col=&#39;red&#39;) legend(&quot;topright&quot;,legend=c(&quot;state 1&quot;,&quot;state 2&quot;),col=c(&quot;red&quot;,&quot;blue&quot;),pch=1) 4.2 Generate 250 simulations from the fitted HMM This involves generating the state sequence from the transition probability matrix and resampling flows from the corresponding component distribution. # Now simulate # First simulate a sequence of states from the TPM # simulate from the transition probability N = length(x) nsim = 250 nprob = length(decoding[decoding == 1])/N delta1=c(nprob,1-nprob) #stationary probability zsim = mchain(NULL,hmm$Pi,delta=delta1) may.sim = matrix(0,nrow=nsim,ncol=N) # Points where May PDF is evaluated xeval=seq(min(flow$may)-0.25*sd(flow$may), max(flow$may)+0.25*sd(flow$may),length=100) sim.pdf=matrix(0,nrow=nsim,ncol=100) # Array to store May simulated PDF may.stat=matrix(NA,ncol = 6,nrow = nsim) # year statistics colnames(may.stat) = c(&quot;mean&quot;,&quot;stdev&quot;,&quot;min&quot;,&quot;max&quot;,&quot;skew&quot;,&quot;cor&quot;) for(isim in 1:nsim){ zsim = simulate(zsim,nsim=N) ## now simulate the flows from the corresponding PDF flowsim = c() for(i in 1:N){ if(zsim$mc[i] == 1)xx=rgamma(1,shape=hmm$pm$shape[1], scale=1/hmm$pm$rate[1]) if(zsim$mc[i] == 2)xx=rgamma(1,shape=hmm$pm$shape[2], scale=1/hmm$pm$rate[2]) flowsim=c(flowsim,xx) } may.sim[isim,]=flowsim sim.pdf[isim,]=sm.density(flowsim,eval.points=xeval, display=&quot;none&quot;)$estimate # fill statistics may.stat[isim,&quot;mean&quot;]=mean(flowsim) may.stat[isim,&quot;max&quot;]=max(flowsim) may.stat[isim,&quot;min&quot;]=min(flowsim) may.stat[isim,&quot;stdev&quot;]=sd(flowsim) may.stat[isim,&quot;skew&quot;]=skewness(flowsim) may.stat[isim,&quot;cor&quot;]=cor(flowsim[-N],flowsim[2:N]) } The statistics from the historical data is added in the first row of the statistics matrix. # Compute statistics from the historical data. obs=1:6 obs[1]=mean(flow$may) obs[2]=sd(flow$may) obs[3]=min(flow$may) obs[4]=max(flow$may) obs[5]=skewness(flow$may) obs[6]=cor(flow$may[-N],flow$may[2:N]) # bind the stats of the historic data at the top.. may.stat=rbind(obs,may.stat) 4.3 Boxplot the resulting statistics We include the mean, variance, skew, lag-1 correlation, minimum, maximum and PDF from the simulations with the corresponding values from the historical data plotted on them. # function to plot boxplots with the structure: hist. in first row plot.bp = function(matrix,name){ xmeans=as.matrix(matrix) n=length(xmeans[,1]) xmeans1=as.matrix(xmeans[2:n,]) #the first row is the original data xs=1:12 zz=boxplot(split(xmeans1,col(xmeans1)), plot=F, cex=1.0) zz$names=rep(&quot;&quot;,length(zz$names)) z1=bxp(zz,ylim=range(xmeans),xlab=&quot;&quot;,ylab=&quot;&quot;,cex=1.00) points(z1,xmeans[1,],pch=16, col=&quot;red&quot;) lines(z1,xmeans[1,],pch=16, col=&quot;gray&quot;) title(main=name) } par(mfrow=c(2,3)) plot.bp(may.stat[,&quot;mean&quot;],&quot;Mean&quot;) plot.bp(may.stat[,&quot;stdev&quot;],&quot;Standard Deviation&quot;) plot.bp(may.stat[,&quot;min&quot;],&quot;Min&quot;) plot.bp(may.stat[,&quot;max&quot;],&quot;Max&quot;) plot.bp(may.stat[,&quot;skew&quot;],&quot;Skews&quot;) plot.bp(may.stat[,&quot;cor&quot;],&quot;Lag-1 correlation&quot;) xdensityorig = flow$may %&gt;% sm.density(.,eval.points=xeval,display=&quot;none&quot;) %&gt;% .$estimate plot.pdf = function(eval,histPDF,simPDF){ xeval = eval plot(xeval,histPDF,pch=&quot;.&quot;,col=&quot;red&quot;,ylim=range(simPDF,histPDF), xlab=&quot;&quot;,ylab = &quot;&quot;) for(i in 1:nsim)lines(xeval,simPDF[i,],col=&#39;lightgrey&#39;,lty=3) lines(xeval,histPDF,lwd=3,col=&quot;red&quot;) title(main=&quot;Historical vs. simulated PDF&quot;) } plot.pdf(xeval,xdensityorig,sim.pdf) The model is able to represent accurately all statistics, including lag-1 correlation. 4.4 Fit a GLM for the state series Instead of simulating the streamflow from a “static” state sequence simulation, the goal is to simulate the state as a prediction from a best logistic GLM. The uncertainty is added choosing random values and comparing them to the initial probability (p1 / p2) of being in a particular state. enso=read.table(&quot;C:/Users/alexb/Google Drive/CVEN 6833 ADAT/zz Homeworks/HW3/enso.txt&quot;) amo=read.table(&quot;C:/Users/alexb/Google Drive/CVEN 6833 ADAT/zz Homeworks/HW3/amo.txt&quot;) pdo=read.table(&quot;C:/Users/alexb/Google Drive/CVEN 6833 ADAT/zz Homeworks/HW3/pdo.txt&quot;) X = data.frame(decoding[-N]-1,enso[-N,],amo[-N,],pdo[-N,]) colnames(X) = c(&quot;ts1&quot;,&quot;enso&quot;,&quot;amo&quot;,&quot;pdo&quot;) Y = decoding[-1]-1 links = c(&quot;logit&quot;, &quot;probit&quot;, &quot;cauchit&quot;,&quot;log&quot;,&quot;cloglog&quot;) # potential links comb=leaps(X,Y, nbest=40,method=&quot;adjr2&quot;)$which # all combinations of cov. aic &lt;- matrix(1e6,ncol=length(links),nrow = length(comb[,1])) colnames(aic) = links[1:length(links)] for(k in 1:length(comb[,1])){ # try every link f. with every comb. xx = X[,comb[k,]] %&gt;% as.data.frame(.) for(i in 1:length(links)){ zz=try(glm(Y ~ ., data=xx, family = binomial(link=links[i]), maxit=500),silent=TRUE) if(class(zz)[1]!=&quot;try-error&quot;)aic[k,i]=zz$aic[1] } } head(aic) ## logit probit cauchit log cloglog ## [1,] 125.3869 125.3869 125.3869 125.3869 125.3869 ## [2,] 146.9211 146.8480 147.3203 145.5355 146.4292 ## [3,] 148.3173 148.4183 147.6240 1000000.0000 148.6837 ## [4,] 151.5616 151.5576 151.5843 151.5075 151.5440 ## [5,] 124.8050 124.6801 125.4507 1000000.0000 124.3818 ## [6,] 125.5837 125.6196 125.6204 1000000.0000 125.7163 index = which(aic == min(aic), arr.ind = TRUE) # select min. AIC print( sprintf(&quot;Choosing the GLM which minimizes AIC for binomial family: %s link function and %s covariates&quot;,links[index[,&quot;col&quot;]], paste(colnames(X)[comb[index[,&quot;row&quot;],]],collapse = &#39;, &#39;))) ## [1] &quot;Choosing the GLM which minimizes AIC for binomial family: cloglog link function and ts1, amo covariates&quot; state.glm = glm(Y ~ ., data=X[,comb[index[,&quot;row&quot;],]], family = binomial(link=links[index[,&quot;col&quot;]])) summary(state.glm) # Model selected ## ## Call: ## glm(formula = Y ~ ., family = binomial(link = links[index[, &quot;col&quot;]]), ## data = X[, comb[index[, &quot;row&quot;], ]]) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.0521 -0.8416 0.4986 0.7478 1.7263 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.9698 0.2798 -3.466 0.000528 *** ## ts1 1.4190 0.3205 4.427 9.55e-06 *** ## amo -1.0552 0.5872 -1.797 0.072305 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 148.06 on 109 degrees of freedom ## Residual deviance: 118.38 on 107 degrees of freedom ## AIC: 124.38 ## ## Number of Fisher Scoring iterations: 6 ENSO and PDO covariates resulted to be insignificant for the logistic regression, even if several laggings from the original data have been tried. 4.5 Use the state GLM to simulate flows from the component distribution With the fitted values from the logistic regression, we calculate first the probabilities (p1, p2) of each state. These are used to generate a random initial state and simulate the rest of the state sequence. Depending on the simulated sequence, a random gamma value is taken from either state 1 or state 2, creating a new streamflow time series, with the same length as the original data. The simulation is repeated 250 to generate statistics and ensemble PDF. X.new = X[,comb[index[,&quot;row&quot;],]] p1 = 1 - sum(round(state.glm$fitted.values))/length(state.glm$fitted.values) state = 1:N # init. state state.p1 = 1:nsim # store p1 for debugging may.sim.glm = matrix(0,nrow=nsim,ncol=N) sim.pdf.glm = matrix(0,nrow=nsim,ncol=100) # Array to store May simulated PDF may.stat.glm=matrix(NA,ncol = 6,nrow = nsim) # statistics colnames(may.stat.glm) = c(&quot;mean&quot;,&quot;stdev&quot;,&quot;min&quot;,&quot;max&quot;,&quot;skew&quot;,&quot;cor&quot;) # simulation for(isim in 1:nsim){ # for(j in 2:N)state[j]=round(predict(state.glm,newdata = data.frame(ts1=state[j-1],amo = amo[j-1,1]))) for(j in 1:N){ if(j==1){ state[1]=ifelse(runif(1)&lt;p1,1,2) }else{ state[j]=ifelse(state.glm$fitted.values[j-1]&lt;runif(1),1,2) } if(state[j]==1){ may.sim.glm[isim,j]=rgamma(1,shape = hmm$pm$shape[1],hmm$pm$rate[1]) }else{ may.sim.glm[isim,j]=rgamma(1,shape = hmm$pm$shape[2],hmm$pm$rate[2]) } } state.p1[isim] = 1 - sum((state-1))/length(state) sim.pdf.glm[isim,]=sm.density(may.sim.glm[isim,], eval.points=xeval,display=&quot;none&quot;)$estimate # fill statistics may.stat.glm[isim,&quot;mean&quot;]=mean(may.sim.glm[isim,]) may.stat.glm[isim,&quot;max&quot;]=max(may.sim.glm[isim,]) may.stat.glm[isim,&quot;min&quot;]=min(may.sim.glm[isim,]) may.stat.glm[isim,&quot;stdev&quot;]=sd(may.sim.glm[isim,]) may.stat.glm[isim,&quot;skew&quot;]=skewness(may.sim.glm[isim,]) may.stat.glm[isim,&quot;cor&quot;]=cor(may.sim.glm[isim,-N],may.sim.glm[isim,2:N]) } # bind the stats of the historic data at the top.. may.stat.glm=rbind(obs,may.stat.glm) par(mfrow=c(2,3)) plot.bp(may.stat.glm[,&quot;mean&quot;],&quot;Mean&quot;) plot.bp(may.stat.glm[,&quot;stdev&quot;],&quot;Standard Deviation&quot;) plot.bp(may.stat.glm[,&quot;min&quot;],&quot;Min&quot;) plot.bp(may.stat.glm[,&quot;max&quot;],&quot;Max&quot;) plot.bp(may.stat.glm[,&quot;skew&quot;],&quot;Skews&quot;) plot.bp(may.stat.glm[,&quot;cor&quot;],&quot;Lag-1 correlation&quot;) plot.pdf(xeval,xdensityorig,sim.pdf.glm) Functions from lib.R: # source functions # takend from &quot;http://civil.colorado.edu/~balajir/CVEN6833/R-sessions/session3/files-4HW3/lib.R&quot; Pi_init &lt;- function(n,type=&#39;uniform&#39;){ matrix(rep(1/n,n^2),n)} delta_init &lt;- function(n, type=&#39;uniform&#39;){ d &lt;- rnorm(n)^2 d/sum(d)} ntile.ts &lt;- function(x, n, limit.type = &#39;prob&#39;, tie = 1, altobs = NULL ){ # returns an integer vector corresponding to n states broken by equal # probability or equal distance # limit &lt;- if(limit.type == &#39;prob&#39;) quantile(x,seq(0,1,1/n)) else if(limit.type == &#39;equal&#39;) seq(min(x),max(x),by=diff(range(x))/n) if(!is.null(altobs)) limit &lt;- quantile(altobs,seq(0,1,1/n)) b &lt;- integer(length(x)) for(i in 1:(n+1)){ filter &lt;- if(tie == 1) x &gt;= limit[i] &amp; x &lt;= limit[i+1] else x &gt; limit[i] &amp; x &lt;= limit[i+1] #only need to set the 1&#39;s because b is already 0&#39;s b[filter] &lt;- as.integer(i-1) } if(class(x) == &#39;ts&#39;) return(ts(b,start=start(x),end=end(x))) else return(b) } get.named.parlist &lt;- function(x,m,dist,ic,...){ require(MASS) fit &lt;- fitdistr(x,dist,...) np &lt;- length(fit$estimate) pars &lt;- vector(&#39;list&#39;,np) names(pars) &lt;- names(fit$estimate) init &lt;- lapply(fit$estimate,max) names(init) &lt;- names(fit$estimate) for(j in 1:m){ #print(j) #browser() #browser() this.fit &lt;- fitdistr(x[ntile.ts(x,m) == (j-1)],dist,init,...) #for(k in 1:np) # pars[[k]][j] &lt;- this.fit$estimate[k] for(k in 1:np) pars[[k]][j] &lt;- fit$estimate[k] if(dist == &#39;normal&#39;){ if(ic == &#39;same.both&#39;){ pars[[k]][j] &lt;- mean(x) pars[[k]][j] &lt;- sd(x) } else if( ic == &#39;same.sd&#39;){ pars[[k]][j] &lt;- mean(x[ntile.ts(x,m) == (j-1)]) pars[[k]][j] &lt;- sd(x) }else{ pars[[k]][j] &lt;- mean(x[ntile.ts(x,m) == (j-1)]) pars[[k]][j] &lt;- sd(x[ntile.ts(x,m) == (j-1)]) } } } pars } AIC.dthmm &lt;- function(x){ ## Return the Akaieke Information criterion value for a fitted discrete ## time hidden markov model from the HiddenMarkov package # Model order m &lt;- length(x$delta) # Log Liklihood value LL &lt;- x$LL # number of parameters p &lt;- m+m^2 # AIC -2*LL + 2*p } ggplot_stationary_hmm &lt;- function(x,binwidth=NULL,res=1000,cols=NULL,...){ m &lt;- length(x$delta) dens &lt;- matrix(0,nrow=m+1,ncol=res) r &lt;- extendrange(x$x,f=.05) xrange &lt;- seq(r[1],r[2],len=res) delta &lt;- statdist(x$Pi) if(is.null(binwidth)) binwidth &lt;- diff(range(x$x))/8 for(i in 1:m){ if(x$distn == &#39;gamma&#39;){ dens[i,] &lt;- delta[i]*dgamma(xrange,shape=x$pm$shape[i],rate=x$pm$rate[i]) }else if(x$distn == &#39;norm&#39;){ dens[i,] &lt;- delta[i]*dnorm(xrange,mean=x$pm$mean[i],sd=x$pm$sd[i]) }else{ stop(&#39;Distribution not supported&#39;) } dens[m+1,] &lt;- dens[m+1,] + dens[i,] } p &lt;- ggplot()+ geom_histogram(data=data.frame(x=as.vector(x$x)),aes(x=x,y=..density..), binwidth=binwidth,fill=&#39;white&#39;,color=&#39;black&#39;)+ theme_bw() dt &lt;- data.table(x=numeric(0),y=numeric(0), state=integer(0)) for(i in 1:m) dt &lt;- rbind(dt, data.table(x=xrange,y=dens[i,], state=i)) dt$state &lt;- factor(dt$state) p &lt;- p + geom_line(data=dt,aes(x=x,y=y,color=state)) + geom_line(data=data.frame(x=xrange,y=dens[m+1,]),aes(x=x,y=y),color=&#39;black&#39;,size=1) + scale_color_tableau() + scale_x_continuous(limits=r) p } statdist &lt;- function(tpm){ m &lt;- nrow(tpm) ones &lt;- rbind(rep(1,m)) I &lt;- diag(rep(1,m)) U &lt;- matrix(rep(1,m^2),m) as.vector(ones %*% solve(I - tpm + U)) } "]
]
