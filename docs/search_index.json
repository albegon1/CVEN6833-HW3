[
["use-of-hidden-markov-models-for-forecasting.html", "Exercise 7 Use of Hidden Markov Models for forecasting 7.1 Fit a best HMM model for the spring season flows 7.2 Generate 250 simulations each of same length as the historical data 7.3 Fit a best GLM to the state sequence 7.4 For each year based on the predictors, obtain the probabilities of the states 7.5 Using these state probabilities, simulate flow from the corresponding state PDFs 7.6 Compute the median and compare with the observed flow. Compute RPSS at the terciles. 7.7 Make blind predictions from 2001 and on 7.8 Functions from lib.R", " Exercise 7 Use of Hidden Markov Models for forecasting For the spring season flow (Apr-Jun average) at Lees Ferry on the Colorado River, use HMM to develop a forecasting model, as follows: 7.1 Fit a best HMM model for the spring season flows First we load libraries, data, and functions as in exercise 4. set.seed(1) # allow repetition of results # libraries libr=c(&quot;HiddenMarkov&quot;,&quot;ggplot2&quot;,&quot;data.table&quot;,&quot;ggthemes&quot;, &quot;magrittr&quot;,&quot;sm&quot;,&quot;moments&quot;,&quot;MASS&quot;,&quot;leaps&quot;,&quot;verification&quot;) options(warn=-999) suppressPackageStartupMessages(lapply(libr, require, character.only = TRUE)) # Load flow data mflow = read.table( &quot;http://civil.colorado.edu/~balajir/CVEN6833/HWs/HW-3-2018/LeesFerry-monflows-1906-2016.txt&quot;) x = mflow[,2:13] %&gt;% `rownames&lt;-`(mflow[,1]) %&gt;% # Apr - Jun {rowSums(.[,4:6])} %&gt;% {./10^6} # convert AF to MAF Finding a best HMM implies obtaining the best component (state) PDFs and the state sequence. ## Fit HMM models of orders 1 through 6. Obtain the AIC for each ## Best order is the one with the least value of AIC. family &lt;- &quot;gamma&quot; # underlying distribution for hmm discrete &lt;- FALSE aic1=c() for(imodel in 1:6){ m &lt;- imodel #model order to fit stationary &lt;- F # use a stationary distribution of mixtures # different initial condition types when family == &quot;norm&quot; ic &lt;- &quot;same.sd&quot;#c(&quot;same.sd&quot;,&quot;same.both&quot;,&quot;both.diff&quot;) fd.name &lt;- ifelse(family == &quot;norm&quot;, &quot;normal&quot;, family) Pi &lt;- Pi_init(m) # T.P.M. delta &lt;- delta_init(m) pars &lt;- get.named.parlist(x, m, fd.name, lower=.0, ic)#,start=list(shape1=2,shape2=2)) # set up the model hmm &lt;- dthmm(x, Pi=Pi, delta=delta, family, pars, nonstat=!stationary, discrete = discrete) sink(&quot;p.7.hmm.fit&quot;) if(imodel &lt; 2){ hmm &lt;- BaumWelch(hmm, bwcontrol(maxiter = 1000, posdiff=TRUE,converge = expression(diff &gt; tol))) } else { hmm &lt;- BaumWelch(hmm, bwcontrol(maxiter = 1000, tol = 1e-08)) } sink() # get the hidden states from the fitted model # Global decoding. To get the probability of being in a state: hmm$u decoding &lt;- Viterbi(hmm) # get AIC aic &lt;- AIC(hmm) aic1=c(aic1,aic) } print(aic1) ## [1] 545.0622 544.6791 551.4169 559.6687 570.5841 571.4997 We select the HMM with the lowest AIC. In this case, this happens to be of order 2. We rerun the HMM for best order (m=2) and generate the state sequence (decoding) resulting from it. ## Get the best order bestorder = order(aic1)[1] ## Fit the model for this best order m &lt;- bestorder #model order to fit stationary &lt;- F # use a stationary distribution of mixtures # different initial condition types when family == &quot;norm&quot; ic &lt;- &quot;same.sd&quot;#c(&quot;same.sd&quot;,&quot;same.both&quot;,&quot;both.diff&quot;) fd.name &lt;- ifelse(family == &quot;norm&quot;, &quot;normal&quot;, family) Pi &lt;- Pi_init(m) # T.P.M. delta &lt;- delta_init(m) pars &lt;- get.named.parlist(x, m, fd.name, lower=.0, ic)#,start=list(shape1=2,shape2=2)) # set up the model hmm &lt;- dthmm(x, Pi=Pi, delta=delta, family, pars, nonstat=!stationary, discrete = discrete) sink(&quot;p.7.best.hmm&quot;) hmm &lt;- BaumWelch(hmm, bwcontrol(maxiter = 1000, tol = 1e-08)) sink() # end hidding output decoding &lt;- Viterbi(hmm) The model summary is also attached together with the state probabilities and state sequence plots. print(summary(hmm)) ## $delta ## [1] 1.0000e+00 3.5938e-72 ## ## $Pi ## [,1] [,2] ## [1,] 0.7377182 0.2622818 ## [2,] 0.3051391 0.6948609 ## ## $nonstat ## [1] TRUE ## ## $distn ## [1] &quot;gamma&quot; ## ## $pm ## $pm$rate ## [1] 1.990288 1.129834 ## ## $pm$shape ## [1] 19.253742 7.583725 ## ## ## $discrete ## [1] FALSE ## ## $n ## [1] 111 cat(&#39;Model order:&#39;,m,&#39;\\n&#39;) ## Model order: 2 p &lt;- ggplot_stationary_hmm(hmm,.5) print(p) state.1=ifelse(decoding==1,mean(x)-1,NA) state.2=ifelse(decoding==2,mean(x)+1,NA) plot(x,type=&quot;l&quot;,ylab=&quot;&quot;, main=&quot;Modified Spring flows and states (supperposed)&quot;) points(state.2,col=&#39;blue&#39;) points(state.1,col=&#39;red&#39;) legend(&quot;topright&quot;,legend=c(&quot;state 1&quot;,&quot;state 2&quot;),col=c(&quot;red&quot;,&quot;blue&quot;),pch=1) 7.2 Generate 250 simulations each of same length as the historical data This involves generating the state sequence from the transition probability matrix and resampling flows from the corresponding component distribution. # simulate from the transition probability N = length(x) nsim = 250 nprob = length(decoding[decoding == 1])/N delta1=c(nprob,1-nprob) #stationary probability zsim = mchain(NULL,hmm$Pi,delta=delta1) spring.sim = matrix(0,nrow=nsim,ncol=N) # Points where May PDF is evaluated xeval=seq(min(x)-0.25*sd(x), max(x)+0.25*sd(x),length=100) sim.pdf=matrix(0,nrow=nsim,ncol=100) # Array to store May simulated PDF spring.stat=matrix(NA,ncol = 6,nrow = nsim) # year statistics colnames(spring.stat) = c(&quot;mean&quot;,&quot;stdev&quot;,&quot;min&quot;,&quot;max&quot;,&quot;skew&quot;,&quot;cor&quot;) for(isim in 1:nsim){ zsim = simulate(zsim,nsim=N) ## now simulate the flows from the corresponding PDF flowsim = c() for(i in 1:N){ if(zsim$mc[i] == 1)xx=rgamma(1,shape=hmm$pm$shape[1], scale=1/hmm$pm$rate[1]) if(zsim$mc[i] == 2)xx=rgamma(1,shape=hmm$pm$shape[2], scale=1/hmm$pm$rate[2]) flowsim=c(flowsim,xx) } spring.sim[isim,]=flowsim sim.pdf[isim,]=sm.density(flowsim,eval.points=xeval, display=&quot;none&quot;)$estimate # fill statistics spring.stat[isim,&quot;mean&quot;]=mean(flowsim) spring.stat[isim,&quot;max&quot;]=max(flowsim) spring.stat[isim,&quot;min&quot;]=min(flowsim) spring.stat[isim,&quot;stdev&quot;]=sd(flowsim) spring.stat[isim,&quot;skew&quot;]=skewness(flowsim) spring.stat[isim,&quot;cor&quot;]=cor(flowsim[-N],flowsim[2:N]) } The statistics from the 250 simulations and the simulated PDF are plotted with the original data. # Compute statistics from the historical data. obs=1:6 obs[1]=mean(x) obs[2]=sd(x) obs[3]=min(x) obs[4]=max(x) obs[5]=skewness(x) obs[6]=cor(x[-N],x[2:N]) # bind the stats of the historic data at the top.. spring.stat=rbind(obs,spring.stat) # function to plot boxplots with the structure: hist. in first row plot.bp = function(matrix,name){ xmeans=as.matrix(matrix) n=length(xmeans[,1]) xmeans1=as.matrix(xmeans[2:n,]) #the first row is the original data xs=1:12 zz=boxplot(split(xmeans1,col(xmeans1)), plot=F, cex=1.0) zz$names=rep(&quot;&quot;,length(zz$names)) z1=bxp(zz,ylim=range(xmeans),xlab=&quot;&quot;,ylab=&quot;&quot;,cex=1.00) points(z1,xmeans[1,],pch=16, col=&quot;red&quot;) lines(z1,xmeans[1,],pch=16, col=&quot;gray&quot;) title(main=name) } par(mfrow=c(2,3)) plot.bp(spring.stat[,&quot;mean&quot;],&quot;Mean&quot;) plot.bp(spring.stat[,&quot;stdev&quot;],&quot;Standard Deviation&quot;) plot.bp(spring.stat[,&quot;min&quot;],&quot;Min&quot;) plot.bp(spring.stat[,&quot;max&quot;],&quot;Max&quot;) plot.bp(spring.stat[,&quot;skew&quot;],&quot;Skews&quot;) plot.bp(spring.stat[,&quot;cor&quot;],&quot;Lag-1 correlation&quot;) xdensityorig = x %&gt;% sm.density(.,eval.points=xeval,display=&quot;none&quot;) %&gt;% .$estimate plot.pdf = function(eval,histPDF,simPDF){ xeval = eval plot(xeval,histPDF,pch=&quot;.&quot;,col=&quot;red&quot;,ylim=range(simPDF,histPDF), xlab=&quot;&quot;,ylab = &quot;&quot;) for(i in 1:nsim)lines(xeval,simPDF[i,],col=&#39;lightgrey&#39;,lty=3) lines(xeval,histPDF,lwd=3,col=&quot;red&quot;) title(main=&quot;Historical vs. simulated PDF&quot;) } plot.pdf(xeval,xdensityorig,sim.pdf) 7.3 Fit a best GLM to the state sequence Use the predictor as a function of preceeding winter season climate indices and the state from the previous year. enso=read.table(&quot;C:/Users/alexb/Google Drive/CVEN 6833 ADAT/zz Homeworks/HW3/enso.txt&quot;) amo=read.table(&quot;C:/Users/alexb/Google Drive/CVEN 6833 ADAT/zz Homeworks/HW3/amo.txt&quot;) pdo=read.table(&quot;C:/Users/alexb/Google Drive/CVEN 6833 ADAT/zz Homeworks/HW3/pdo.txt&quot;) X = data.frame(decoding[-N]-1,enso[-N,],amo[-N,],pdo[-N,]) colnames(X) = c(&quot;ts1&quot;,&quot;enso&quot;,&quot;amo&quot;,&quot;pdo&quot;) Y = decoding[-1]-1 links = c(&quot;logit&quot;, &quot;probit&quot;, &quot;cauchit&quot;,&quot;log&quot;,&quot;cloglog&quot;) # potential links comb=leaps(X,Y, nbest=40,method=&quot;adjr2&quot;)$which # all combinations of cov. aic &lt;- matrix(1e6,ncol=length(links),nrow = length(comb[,1])) colnames(aic) = links[1:length(links)] for(k in 1:length(comb[,1])){ # try every link f. with every comb. xx = X[,comb[k,]] %&gt;% as.data.frame(.) for(i in 1:length(links)){ zz=try(glm(Y ~ ., data=xx, family = binomial(link=links[i]), maxit=500),silent=TRUE) if(class(zz)[1]!=&quot;try-error&quot;)aic[k,i]=zz$aic[1] } } head(aic) ## logit probit cauchit log cloglog ## [1,] 97.74487 97.74487 97.74487 9.774487e+01 97.74487 ## [2,] 126.88566 126.47855 128.77622 1.000000e+06 128.60526 ## [3,] 148.52354 148.52630 148.52114 1.485643e+02 148.53909 ## [4,] 148.87507 148.86830 148.91355 1.489096e+02 148.89203 ## [5,] 92.64201 92.79124 91.54990 1.000000e+06 93.72228 ## [6,] 99.22276 99.27680 98.72933 1.000000e+06 98.71944 index = which(aic == min(aic), arr.ind = TRUE) # select min. AIC print( sprintf(paste(&quot;Choosing the GLM which minimizes AIC for binomial&quot;, &quot;family: %s link function and %s covariates&quot;), links[index[,&quot;col&quot;]], paste(colnames(X)[comb[index[,&quot;row&quot;],]],collapse = &#39;, &#39;))) ## [1] &quot;Choosing the GLM which minimizes AIC for binomial family: cauchit link function and ts1, amo covariates&quot; state.glm = glm(Y ~ ., data=X[,comb[index[,&quot;row&quot;],]], family = binomial(link=links[index[,&quot;col&quot;]])) summary(state.glm) # Model selected ## ## Call: ## glm(formula = Y ~ ., family = binomial(link = links[index[, &quot;col&quot;]]), ## data = X[, comb[index[, &quot;row&quot;], ]]) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2583 -0.4897 -0.3741 0.5019 2.2797 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.8665 0.9469 -3.027 0.00247 ** ## ts1 3.9808 1.2182 3.268 0.00108 ** ## amo 6.1710 2.5808 2.391 0.01680 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 145.29 on 109 degrees of freedom ## Residual deviance: 85.55 on 107 degrees of freedom ## AIC: 91.55 ## ## Number of Fisher Scoring iterations: 6 To evaluate if the difference of the variance of the model is significantly better we will perform an ANOVA (via Likelihood Ratio Test) of the best model vs. the most complex model (4 covariates) with the lowest AIC. glm.complex = glm(Y ~ ., data=X, family = binomial(link=&quot;cauchit&quot;), maxit=500) anova(glm.complex,state.glm, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: Y ~ ts1 + enso + amo + pdo ## Model 2: Y ~ ts1 + amo ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 105 84.252 ## 2 107 85.550 -2 -1.2984 0.5225 The variance is not significantly better and we accept H0 hypothesis. We retain the most complex model. state.glm=glm.complex 7.4 For each year based on the predictors, obtain the probabilities of the states We have used a binomial glm where state 1 = 0 and state 2 = 1. Therefore, the fitted values (ranging from 0 to 1) represent the probability of of being on state 2. plot(state.glm$fitted.values) 7.5 Using these state probabilities, simulate flow from the corresponding state PDFs state.p = 1:N # init. state probabilities state.p[1] = sum(round(state.glm$fitted.values))/length(state.glm$fitted.values) # 1906 state.p[2:N] = state.glm$fitted.values # 1907 - 2016 state.sim = 1:N # init. simulated state flow.sim.glm = matrix(0,nrow=nsim,ncol=N) sim.pdf.glm = matrix(0,nrow=nsim,ncol=100) # Array to store May simulated PDF spring.stat.glm=matrix(NA,ncol = 6,nrow = nsim) # statistics colnames(spring.stat.glm) = c(&quot;mean&quot;,&quot;stdev&quot;,&quot;min&quot;,&quot;max&quot;,&quot;skew&quot;,&quot;cor&quot;) # simulation for(isim in 1:nsim){ for(j in 1:N){ state.sim[j]=ifelse(runif(1)&lt;state.p[j],2,1) if(state.sim[j]==1){ flow.sim.glm[isim,j]=rgamma(1,shape = hmm$pm$shape[1],hmm$pm$rate[1]) }else{ flow.sim.glm[isim,j]=rgamma(1,shape = hmm$pm$shape[2],hmm$pm$rate[2]) } } sim.pdf.glm[isim,]=sm.density(flow.sim.glm[isim,], eval.points=xeval,display=&quot;none&quot;)$estimate # fill statistics spring.stat.glm[isim,&quot;mean&quot;]=mean(flow.sim.glm[isim,]) spring.stat.glm[isim,&quot;max&quot;]=max(flow.sim.glm[isim,]) spring.stat.glm[isim,&quot;min&quot;]=min(flow.sim.glm[isim,]) spring.stat.glm[isim,&quot;stdev&quot;]=sd(flow.sim.glm[isim,]) spring.stat.glm[isim,&quot;skew&quot;]=skewness(flow.sim.glm[isim,]) spring.stat.glm[isim,&quot;cor&quot;]=cor(flow.sim.glm[isim,-N],flow.sim.glm[isim,2:N]) } # bind the stats of the historic data at the top.. spring.stat.glm=rbind(obs,spring.stat.glm) par(mfrow=c(2,3)) plot.bp(spring.stat.glm[,&quot;mean&quot;],&quot;Mean&quot;) plot.bp(spring.stat.glm[,&quot;stdev&quot;],&quot;Standard Deviation&quot;) plot.bp(spring.stat.glm[,&quot;min&quot;],&quot;Min&quot;) plot.bp(spring.stat.glm[,&quot;max&quot;],&quot;Max&quot;) plot.bp(spring.stat.glm[,&quot;skew&quot;],&quot;Skews&quot;) plot.bp(spring.stat.glm[,&quot;cor&quot;],&quot;Lag-1 correlation&quot;) plot.pdf(xeval,xdensityorig,sim.pdf.glm) 7.6 Compute the median and compare with the observed flow. Compute RPSS at the terciles. The median values are compared with the observed values in a scatter plot. The correlation is also calculated. median.sim = 1:N for(i in 1:N) median.sim[i] = median(flow.sim.glm[,i]) # par(mfrow=c(1,2)) plot(x,type=&quot;l&quot;) lines(median.sim,col = &quot;red&quot;) plot(x,median.sim,xlab=&quot;Observed flows&quot;,ylab = &quot;Simulated median flows&quot;) The time series plot shows persistance on the simulated median values, with two clear states. The calculated correlation between observation and simulated median values is: cor(x,median.sim) ## [1] 0.4417929 The Ranked Probability Skill Score at the terciles is: obs = quantile(x,c(0.333,0.6667)) # flow at given terciles pred.prob = matrix(NA,ncol = nsim, nrow = 2) # Sim. Prob. at observation points baseline = c(0.333,0.6667) # Terciles for(i in 1:nsim){ aux.CDF=ecdf(flow.sim.glm[i,]) # ecdf for each simulation pred.prob[,i] = aux.CDF(obs) # cum. prob. of given flow acc. to ecdf } rps(obs,pred.prob,baseline = baseline)$rpss # RPSS ## [1] -0.08106093 The negative value indicates that the flowstream forecast at the terciles is not improved when using climate-related variables (real probabilities vs. binomial regression prob.). 7.7 Make blind predictions from 2001 and on Evaluate the performance by computing the skills as above. xx = X[100:110,] # covariates 2000 - 2010 for glm blind.flow = matrix(NA,nrow = nsim, ncol = 11) #init sim flows for(isim in 1:nsim){ s = xx[1,&quot;ts1&quot;] + 1 # initial state for the loop for(i in 1:11){ xx[i,&quot;ts1&quot;] = s-1 # replace hmm step seq. with simulated step sequence y = predict(state.glm,newdata = xx[i,],type = &quot;response&quot;) # prob of state 2 s = ifelse(runif(1)&lt;y,2,1) # state f = rgamma(1,shape=hmm$pm$shape[s],scale=1/hmm$pm$rate[s])# streamflow blind.flow[isim,i] = f # assign to time series } } boxplot(blind.flow) title(&quot;Forecasted vs. real streamflows&quot;) lines(x[101:111],col=&quot;red&quot;) points(x[101:111],col=&quot;red&quot;) 7.7.1 Correlation of median streamflows blind.median = 1:11 for(i in 1:11){ blind.median[i] = median(blind.flow[,i]) } plot(x[101:111],blind.median) cor(x[101:111],blind.median) ## [1] -0.06366088 The correlation is significantly lower than the one obtained for the historical time series. 7.7.2 RPSS obs = quantile(x[101:111],c(0.333,0.6667)) # flow at given terciles pred.prob = matrix(NA,ncol = nsim, nrow = 2) # Sim. Prob. at observation points baseline = c(0.333,0.6667) # Terciles for(i in 1:nsim){ aux.CDF=ecdf(blind.flow[i,]) # ecdf for each simulation pred.prob[,i] = aux.CDF(obs) # cum. prob. of given flow acc. to ecdf } rps(obs,pred.prob,baseline = baseline)$rpss # RPSS ## [1] -0.3372899 The score is even more negative, showing the lack of improvement accuracy when forecasting. 7.8 Functions from lib.R # source functions # takend from &quot;http://civil.colorado.edu/~balajir/CVEN6833/R-sessions/session3/files-4HW3/lib.R&quot; Pi_init &lt;- function(n,type=&#39;uniform&#39;){ matrix(rep(1/n,n^2),n)} delta_init &lt;- function(n, type=&#39;uniform&#39;){ d &lt;- rnorm(n)^2 d/sum(d)} ntile.ts &lt;- function(x, n, limit.type = &#39;prob&#39;, tie = 1, altobs = NULL ){ # returns an integer vector corresponding to n states broken by equal # probability or equal distance # limit &lt;- if(limit.type == &#39;prob&#39;) quantile(x,seq(0,1,1/n)) else if(limit.type == &#39;equal&#39;) seq(min(x),max(x),by=diff(range(x))/n) if(!is.null(altobs)) limit &lt;- quantile(altobs,seq(0,1,1/n)) b &lt;- integer(length(x)) for(i in 1:(n+1)){ filter &lt;- if(tie == 1) x &gt;= limit[i] &amp; x &lt;= limit[i+1] else x &gt; limit[i] &amp; x &lt;= limit[i+1] #only need to set the 1&#39;s because b is already 0&#39;s b[filter] &lt;- as.integer(i-1) } if(class(x) == &#39;ts&#39;) return(ts(b,start=start(x),end=end(x))) else return(b) } get.named.parlist &lt;- function(x,m,dist,ic,...){ require(MASS) fit &lt;- fitdistr(x,dist,...) np &lt;- length(fit$estimate) pars &lt;- vector(&#39;list&#39;,np) names(pars) &lt;- names(fit$estimate) init &lt;- lapply(fit$estimate,max) names(init) &lt;- names(fit$estimate) for(j in 1:m){ #print(j) #browser() #browser() this.fit &lt;- fitdistr(x[ntile.ts(x,m) == (j-1)],dist,init,...) #for(k in 1:np) # pars[[k]][j] &lt;- this.fit$estimate[k] for(k in 1:np) pars[[k]][j] &lt;- fit$estimate[k] if(dist == &#39;normal&#39;){ if(ic == &#39;same.both&#39;){ pars[[k]][j] &lt;- mean(x) pars[[k]][j] &lt;- sd(x) } else if( ic == &#39;same.sd&#39;){ pars[[k]][j] &lt;- mean(x[ntile.ts(x,m) == (j-1)]) pars[[k]][j] &lt;- sd(x) }else{ pars[[k]][j] &lt;- mean(x[ntile.ts(x,m) == (j-1)]) pars[[k]][j] &lt;- sd(x[ntile.ts(x,m) == (j-1)]) } } } pars } AIC.dthmm &lt;- function(x){ ## Return the Akaieke Information criterion value for a fitted discrete ## time hidden markov model from the HiddenMarkov package # Model order m &lt;- length(x$delta) # Log Liklihood value LL &lt;- x$LL # number of parameters p &lt;- m+m^2 # AIC -2*LL + 2*p } ggplot_stationary_hmm &lt;- function(x,binwidth=NULL,res=1000,cols=NULL,...){ m &lt;- length(x$delta) dens &lt;- matrix(0,nrow=m+1,ncol=res) r &lt;- extendrange(x$x,f=.05) xrange &lt;- seq(r[1],r[2],len=res) delta &lt;- statdist(x$Pi) if(is.null(binwidth)) binwidth &lt;- diff(range(x$x))/8 for(i in 1:m){ if(x$distn == &#39;gamma&#39;){ dens[i,] &lt;- delta[i]*dgamma(xrange,shape=x$pm$shape[i],rate=x$pm$rate[i]) }else if(x$distn == &#39;norm&#39;){ dens[i,] &lt;- delta[i]*dnorm(xrange,mean=x$pm$mean[i],sd=x$pm$sd[i]) }else{ stop(&#39;Distribution not supported&#39;) } dens[m+1,] &lt;- dens[m+1,] + dens[i,] } p &lt;- ggplot()+ geom_histogram(data=data.frame(x=as.vector(x$x)),aes(x=x,y=..density..), binwidth=binwidth,fill=&#39;white&#39;,color=&#39;black&#39;)+ theme_bw() dt &lt;- data.table(x=numeric(0),y=numeric(0), state=integer(0)) for(i in 1:m) dt &lt;- rbind(dt, data.table(x=xrange,y=dens[i,], state=i)) dt$state &lt;- factor(dt$state) p &lt;- p + geom_line(data=dt,aes(x=x,y=y,color=state)) + geom_line(data=data.frame(x=xrange,y=dens[m+1,]),aes(x=x,y=y),color=&#39;black&#39;,size=1) + scale_color_tableau() + scale_x_continuous(limits=r) p } statdist &lt;- function(tpm){ m &lt;- nrow(tpm) ones &lt;- rbind(rep(1,m)) I &lt;- diag(rep(1,m)) U &lt;- matrix(rep(1,m^2),m) as.vector(ones %*% solve(I - tpm + U)) } "]
]
