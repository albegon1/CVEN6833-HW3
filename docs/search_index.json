[
["index.html", "CVEN 6833 - Homework 3 Topics", " CVEN 6833 - Homework 3 Alex Belenguer Topics Parametric/Nonparametric Time Series Hidden Markov Model Wavelet Spectral Analysis Extreme Value Time Series Copulas "],
["seasonal-ar1-model.html", "Exercise 1 Seasonal AR(1) model 1.1 Generate 250 simulations each of same length as the historical data. 1.2 Plot statistics from simulations 1.3 Replace the simulation of the errors (or innovations) from Normal to Gamma", " Exercise 1 Seasonal AR(1) model Fit a seasonal AR(1) model – i.e., nonstationary time series model to the monthly Colorado River Flow at Lees Ferry. 1.1 Generate 250 simulations each of same length as the historical data. # Load libraries libr=c(&quot;magrittr&quot;,&quot;sm&quot;,&quot;stats&quot;,&quot;moments&quot;) options(warn=1) suppressPackageStartupMessages(lapply(libr, require, character.only = TRUE)) # import and set up flow data flow = read.table( &quot;http://civil.colorado.edu/~balajir/CVEN6833/HWs/HW-3-2018/LeesFerry-monflows-1906-2016.txt&quot;) flow = flow[,2:13] %&gt;% `rownames&lt;-`(flow[,1]) %&gt;% setNames(.,c(&quot;jan&quot;,&quot;feb&quot;,&quot;mar&quot;,&quot;apr&quot;,&quot;may&quot;,&quot;jun&quot;, &quot;jul&quot;,&quot;aug&quot;,&quot;sep&quot;,&quot;oct&quot;,&quot;nov&quot;,&quot;dec&quot;)) %&gt;% {./10^6} # convert AF to MAF head(flow,n=1L) # show values ## jan feb mar apr may jun jul aug ## 1906 0.244314 0.292534 0.678174 1.20464 3.635101 5.014167 2.95046 1.605086 ## sep oct nov dec ## 1906 1.503159 0.739807 0.503006 0.353312 tail(flow,n=1L) ## jan feb mar apr may jun jul ## 2016 0.360703 0.448837 0.67914 1.099567 2.967581 3.910287 1.342044 ## aug sep oct nov dec ## 2016 0.609946 0.485507 0.546633 0.426289 0.345163 flow$year = rowSums(flow) # add year in 13th column par(mfrow=c(4,3)) # plot histogram and density of monthly flow for(i in 1:12){ hist(flow[,i], freq=FALSE, main = colnames(flow)[i],xlab = &quot;&quot;,ylab = &quot;&quot;) sm.density(flow[,i], add=TRUE) } par(mfrow=c(1,1)) # plot histogram and density of anual flow hist(flow[,13], freq=FALSE, main = colnames(flow)[13],xlab = &quot;&quot;) sm.density(flow[,13], add=TRUE) The seasonal AR model is fitted using the Thomas Fiering coefficients. The anual flow AR model is fitted using stats::arima. # Get the parameters of the Thomas Fiering Model (12 models, 1 for each transition) coef1 = coef2 = rep(0,length.out = 12) coef1[1] = cor(flow[-1,&quot;jan&quot;],flow[-111,&quot;dec&quot;]) # jan - dec coef2[1] = sqrt((var(flow[,1])) * (1. - coef1[1]*coef1[1])) for(i in 2:12){ # remaining month pairs coef1[i] = cor(flow[,i],flow[,i-1]) coef2[i] = sqrt((var(flow[,i])) * (1. - coef1[i]*coef1[i])) } # The anual flow is modeled using single AR(1) model ar.year=ar(flow$year,order.max = 1) #AR order 1, MA The 12 pairs of the TF coefficients are used to run 250 simulations (synthetic values) that will populate the statistics for the models. For the anual flow model, the simulations are obtained via stats::arima.sim. The random gamma values are related to the normal following the relationship explained in the thread below: https://stats.stackexchange.com/questions/37461/the-relationship-between-the-gamma-distribution-and-the-normal-distribution innovation = &quot;Normal&quot; # defines nature of innovations: &quot;Normal&quot; or &quot;Gamma&quot; # parameters for equivalent gamma distribution # N(x;0,s) ~ lim(a -&gt; +inf) G((a-1)*sqrt(1/a)*s;a,sqrt(1/a)*s) a=5 sm=1 #sd for monthly innovations sy=sd(flow$year) #sd for monthly innovations # peak density for each month peak=rep(NA,12) for(i in 1:12){ aux=sm.density(flow[,i],display=&quot;none&quot;) peak[i]=aux$eval.points[which.max(aux$estimate)]*1.5 } # Simulations (innovation defines st. distribution of error) nsim=250 # number of simulations nyrs=length(flow[,1]) # years armean=matrix(0,nsim,12) #matrices that store the statistics arstdev=matrix(0,nsim,12) arcor=matrix(0,nsim,12) arskw=matrix(0,nsim,12) armax=matrix(0,nsim,12) armin=matrix(0,nsim,12) ar.year.stat=matrix(NA,ncol = 6,nrow = nsim) # year statistics colnames(ar.year.stat) = c(&quot;mean&quot;,&quot;stdev&quot;,&quot;min&quot;,&quot;max&quot;,&quot;skew&quot;,&quot;cor&quot;) # Points where May PDF is evaluated xeval=seq(min(flow$may)-0.25*sd(flow$may), max(flow$may)+0.25*sd(flow$may),length=100) simpdf=matrix(0,nrow=nsim,ncol=100) # Array to store May simulated PDF # Points where anual PDF is evaluated yeval=seq(min(flow$year)-0.25*sd(flow$year), max(flow$year)+0.25*sd(flow$year),length=100) year.pdf=matrix(0,nrow=nsim,ncol=100) # Array to store anual simulated PDF for(k in 1:nsim){ nmons=nyrs*12 #number of values to be generated xsim=1:nmons r=sample(1:nyrs,1) xsim[1]=flow[r,1] # Starting point for sim xprev=xsim[1] for(i in 2:nmons){ j=i %% 12 if(j == 0) j=12 j1=j-1 if(j == 1) j1=12 x1=xprev-ifelse(innovation==&quot;Normal&quot;,mean(flow[,j1]),peak[j1]) x2=coef2[j]*ifelse(innovation==&quot;Normal&quot;,rnorm(1,0,1), rgamma(1,shape=a,scale=sqrt(1/a)*sm)-(a-1)*sqrt(1/a)*sm) xsim[i]=mean(flow[,j]) + x1*coef1[j] + x2 xprev=xsim[i] } #Store simulated values in matrix form, get May values and PDF simdismon=matrix(xsim,ncol = 12, byrow = TRUE) # filled by row maysim = simdismon[,5] # Synthetic values for May simpdf[k,]=sm.density(maysim,eval.points=xeval,display=&quot;none&quot;)$estimate # Fill statistics for each month for(j in 1:12){ armean[k,j]=mean(simdismon[,j]) armax[k,j]=max(simdismon[,j]) armin[k,j]=min(simdismon[,j]) arstdev[k,j]=sd(simdismon[,j]) arskw[k,j]=skewness(simdismon[,j]) } arcor[k,1]=cor(simdismon[-nyrs,12],simdismon[2:nyrs,1]) #cor dec-jan for(j in 2:12){ # rest of pairs j1=j-1 arcor[k,j]=cor(simdismon[,j],simdismon[,j1]) } # anual flow simulations if(innovation==&quot;Normal&quot;){ ar.year.sim = arima.sim(n = nyrs, list(ar = ar.year$ar), sd = sqrt(ar.year$var.pred)) + mean(flow$year) }else{ ar.year.sim = arima.sim(n = nyrs, list(ar = ar.year$ar), rand.gen = function(n, ...) rgamma(n,shape=a, scale=sqrt(1/a)*sy)-(a-1)*sqrt(1/a)*sy) + yeval[which.max(year.density)]*0.85 } # Get anual PDF year.pdf[k,]=sm.density(ar.year.sim,eval.points= yeval,display=&quot;none&quot;)$estimate # Calculate statistics ar.year.stat[k,&quot;mean&quot;]=mean(ar.year.sim) ar.year.stat[k,&quot;max&quot;]=max(ar.year.sim) ar.year.stat[k,&quot;min&quot;]=min(ar.year.sim) ar.year.stat[k,&quot;stdev&quot;]=sd(ar.year.sim) ar.year.stat[k,&quot;skew&quot;]=skewness(ar.year.sim) ar.year.stat[k,&quot;cor&quot;]=cor(ar.year.sim[-nyrs],ar.year.sim[2:nyrs]) } The statistics from the synthetic values and the historical data are bound in the same matrix. # Compute statistics from the historical data. obsmean=1:12 obsstdev=1:12 obscor=1:12 obsskw=1:12 obsmax=1:12 obsmin=1:12 for(i in 1:12){ obsmax[i]=max(flow[,i]) obsmin[i]=min(flow[,i]) obsmean[i]=mean(flow[,i]) obsstdev[i]=sd(flow[,i]) obsskw[i]=skewness(flow[,i]) } obscor[1]= cor(flow[-nyrs,12], flow[2:nyrs,1]) for(i in 2:12){ i1=i-1 obscor[i]=cor(flow[,i], flow[,i1]) } # bind the stats of the historic data at the top.. armean=rbind(obsmean,armean) arstdev=rbind(obsstdev,arstdev) arskw=rbind(obsskw,arskw) arcor=rbind(obscor,arcor) armax=rbind(obsmax,armax) armin=rbind(obsmin,armin) # anual flow binding year.stat=c(mean(flow$year),sd(flow$year),min(flow$year), max(flow$year),skewness(flow$year), cor(flow$year[-nyrs],flow$year[2:nyrs])) ar.year.stat = rbind(year.stat,ar.year.stat) 1.2 Plot statistics from simulations Create boxplots of annual and monthly, mean, variance, skew, lag-1 correlation, minimum, maximum and PDFs of May and annual flows. Comment on what you observe and also on why some of the monthly statistics are not captured. # function to plot boxplots with the structure: hist. in first row plot.bp = function(matrix,name){ xmeans=as.matrix(matrix) n=length(xmeans[,1]) xmeans1=as.matrix(xmeans[2:n,]) #the first row is the original data xs=1:12 zz=boxplot(split(xmeans1,col(xmeans1)), plot=F, cex=1.0) zz$names=rep(&quot;&quot;,length(zz$names)) z1=bxp(zz,ylim=range(xmeans),xlab=&quot;&quot;,ylab=&quot;&quot;,cex=1.00) points(z1,xmeans[1,],pch=16, col=&quot;red&quot;) lines(z1,xmeans[1,],pch=16, col=&quot;gray&quot;) title(main=name) } The plots for the statistics of the simulated time series (shown as boxplots) vs. the historical data (shown as points and lines) are reproduced below: par(mfrow=c(3,2)) plot.bp(armean,&quot;Mean&quot;) plot.bp(arstdev,&quot;Standard Deviation&quot;) plot.bp(armin,&quot;Min&quot;) plot.bp(armax,&quot;Max&quot;) plot.bp(arskw,&quot;Skews&quot;) plot.bp(arcor,&quot;Lag-1 correlation&quot;) The model proficiently captures the mean and max values. A fair fit is obtained with the standard deviation. However, the normality of the innovations results in a poor fit of minimum values and skews. The anual statistics are similarly represented below: par(mfrow=c(2,3)) plot.bp(ar.year.stat[,&quot;mean&quot;],&quot;Mean&quot;) plot.bp(ar.year.stat[,&quot;stdev&quot;],&quot;Standard Deviation&quot;) plot.bp(ar.year.stat[,&quot;min&quot;],&quot;Min&quot;) plot.bp(ar.year.stat[,&quot;max&quot;],&quot;Max&quot;) plot.bp(ar.year.stat[,&quot;skew&quot;],&quot;Skews&quot;) plot.bp(ar.year.stat[,&quot;cor&quot;],&quot;Lag-1 correlation&quot;) The best fitting occurs for mean, sd, and correlation. Min, max and skew hardly contain historical values within the 25th/75th percentile limits. The simulated May PDF vs. the historical May PDF is plotted at 100 points. xdensityorig = flow$may %&gt;% sm.density(.,eval.points=xeval,display=&quot;none&quot;) %&gt;% .$estimate plot.pdf = function(eval,histPDF,simPDF){ xeval = eval plot(xeval,histPDF,pch=&quot;.&quot;,col=&quot;red&quot;,ylim=range(simPDF,histPDF), xlab=&quot;&quot;,ylab = &quot;&quot;) for(i in 1:nsim)lines(xeval,simPDF[i,],col=&#39;lightgrey&#39;,lty=3) lines(xeval,histPDF,lwd=3,col=&quot;red&quot;) title(main=&quot;Historical vs. simulated PDF&quot;) } plot.pdf(xeval,xdensityorig,simpdf) The bimodal historical May PDF is not captured by the simulations due to the Normal nature of the innovations. The simulated vs. historical anual flow PDF is similarly compared. year.density = flow$year %&gt;% sm.density(.,eval.points=yeval, display=&quot;none&quot;) %&gt;% .$estimate plot.pdf(yeval,year.density,year.pdf) 1.3 Replace the simulation of the errors (or innovations) from Normal to Gamma The simulation code chunks are rerun via r markdown code with innovation = “Gamma”. innovation=&quot;Gamma&quot; par(mfrow=c(3,2)) plot.bp(armean,&quot;Mean&quot;) plot.bp(arstdev,&quot;Standard Deviation&quot;) plot.bp(armin,&quot;Min&quot;) plot.bp(armax,&quot;Max&quot;) plot.bp(arskw,&quot;Skews&quot;) plot.bp(arcor,&quot;Lag-1 correlation&quot;) The graphs show a differentiated skew performance, although the fit seems to be equivalent. par(mfrow=c(2,3)) plot.bp(ar.year.stat[,&quot;mean&quot;],&quot;Mean&quot;) plot.bp(ar.year.stat[,&quot;stdev&quot;],&quot;Standard Deviation&quot;) plot.bp(ar.year.stat[,&quot;min&quot;],&quot;Min&quot;) plot.bp(ar.year.stat[,&quot;max&quot;],&quot;Max&quot;) plot.bp(ar.year.stat[,&quot;skew&quot;],&quot;Skews&quot;) plot.bp(ar.year.stat[,&quot;cor&quot;],&quot;Lag-1 correlation&quot;) plot.pdf(xeval,xdensityorig,simpdf) The simulated May PDF is no longer symmetric, as it would be expected from a Gamma Distribution plot.pdf(yeval,year.density,year.pdf) The same effect is depicted in the anual PDF. "],
["nonparametric-seasonal-lag-1-model.html", "Exercise 2 Nonparametric seasonal lag-1 model 2.1 Generate 250 simulations each of same length as the historical data. 2.2 Plot statistics from simulations 2.3 Advantages/disadvantages with a nonparametric approach.", " Exercise 2 Nonparametric seasonal lag-1 model Fit a nonparametric seasonal lag-1 model and repeat [exercise] 1. You can use either the the K-nn bootstrap technique or LOCFIT/residual resampling and repeat 1. 2.1 Generate 250 simulations each of same length as the historical data. # Load libraries libr=c(&quot;magrittr&quot;,&quot;sm&quot;,&quot;stats&quot;,&quot;moments&quot;) options(warn=1) suppressPackageStartupMessages(lapply(libr, require, character.only = TRUE)) # import and set up flow data flow = read.table( &quot;http://civil.colorado.edu/~balajir/CVEN6833/HWs/HW-3-2018/LeesFerry-monflows-1906-2016.txt&quot;) flow = flow[,2:13] %&gt;% `rownames&lt;-`(flow[,1]) %&gt;% setNames(.,c(&quot;jan&quot;,&quot;feb&quot;,&quot;mar&quot;,&quot;apr&quot;,&quot;may&quot;,&quot;jun&quot;, &quot;jul&quot;,&quot;aug&quot;,&quot;sep&quot;,&quot;oct&quot;,&quot;nov&quot;,&quot;dec&quot;)) %&gt;% {./10^6} # convert AF to MAF flow$year = rowSums(flow) # add year in 13th column head(flow,n=1L) # show values ## jan feb mar apr may jun jul aug ## 1906 0.244314 0.292534 0.678174 1.20464 3.635101 5.014167 2.95046 1.605086 ## sep oct nov dec year ## 1906 1.503159 0.739807 0.503006 0.353312 18.72376 tail(flow,n=1L) ## jan feb mar apr may jun jul ## 2016 0.360703 0.448837 0.67914 1.099567 2.967581 3.910287 1.342044 ## aug sep oct nov dec year ## 2016 0.609946 0.485507 0.546633 0.426289 0.345163 13.2217 # Simulations (innovation defines st. distribution of error) nsim=100 # number of simulations nsim1=nsim+1 nyrs=length(flow[,1]) # years nyrs1 = nyrs-1 N=nyrs*12 # time series for monthly flow armean=matrix(0,nsim,12) #matrices that store the statistics arstdev=matrix(0,nsim,12) arcor=matrix(0,nsim,12) arskw=matrix(0,nsim,12) armax=matrix(0,nsim,12) armin=matrix(0,nsim,12) ar.year.stat=matrix(NA,ncol = 6,nrow = nsim) # year statistics colnames(ar.year.stat) = c(&quot;mean&quot;,&quot;stdev&quot;,&quot;min&quot;,&quot;max&quot;,&quot;skew&quot;,&quot;cor&quot;) # Points where May PDF is evaluated xeval=seq(min(flow$may)-0.25*sd(flow$may), max(flow$may)+0.25*sd(flow$may),length=100) simpdf=matrix(0,nrow=nsim,ncol=100) # Array to store May simulated PDF # Points where anual PDF is evaluated yeval=seq(min(flow$year)-0.25*sd(flow$year), max(flow$year)+0.25*sd(flow$year),length=100) year.pdf=matrix(0,nrow=nsim,ncol=100) # Array to store anual simulated PDF # # The anual flow is modeled using single AR(1) model # ar.year=ar(flow$year,order.max = 1) #AR order 1, MA # Fitting Np-AR-1 model K=round(sqrt(nyrs)) #number of nearest neighbors W= 1:K %&gt;% {1/.} %&gt;% {./sum(.)} %&gt;% cumsum(.) # weight function for (isim in 1:nsim) { i=round(runif(1,2,nyrs)) # initial random year zsim = 1:N # n of monthly flow time series zsim[1] = flow[i,1] # initial flow (Jan of year i) xp = zsim[1] # Previous value initialization # Monthly flow simulations for (j in 2:N) { mo = j%%12 # month we are simulating imon=ifelse(mo==0,12,mo) if (mo != 1){ # if mo = 0 (Dec) data = flow[,1:12] # data is the same as test (Jan - Dec) } else { # Jan data = cbind(flow[2:nyrs,1],flow[1:nyrs1,2:12]) } # if mo = 1 (JAn), 1st col of data will be March, last col will be Feb mo1 = mo-1 if(mo == 0)mo1=11 if(mo == 1)mo1=12 xdist = order(abs(xp - data[,mo1])) # if (mo == 1) { # xdist = order(abs(xp - data[,mo1]))} # if (mo &gt;1) { # xx=rbind(xp, data[,(12-ilag):11]) # xdist = order(as.matrix(dist(xx))[1,2:nyrs+1]) } xx=runif(1,0,1) # generate random number xy=c(xx,W) xx=rank(xy) # get rank with respect to W i1=xdist[xx[1]] # distance order at given rank zsim[j]=data[i1,imon] # select data at given distance xp = zsim[j] # store previous year for next it. } # end j loop simdismon = matrix(zsim, ncol=12, byrow = TRUE) # makes a 12 column matrix with jan thru dec maysim = simdismon[,5] simpdf[isim,]=sm.density(maysim,eval.points=xeval,display=&quot;none&quot;)$estimate # Fill statistics for each month for(j in 1:12){ armean[isim,j]=mean(simdismon[,j]) armax[isim,j]=max(simdismon[,j]) armin[isim,j]=min(simdismon[,j]) arstdev[isim,j]=sd(simdismon[,j]) arskw[isim,j]=skewness(simdismon[,j]) } arcor[isim,1]=cor(simdismon[-nyrs,12],simdismon[2:nyrs,1]) #cor dec-jan for(j in 2:12){ # rest of pairs j1=j-1 arcor[isim,j]=cor(simdismon[,j],simdismon[,j1]) } # Anual flow simulations y.sim = 1:nyrs # n of anual flow time series y.sim[1] = flow[i,&quot;year&quot;] # initial flow (Year i) y.p = y.sim[1] # Previous value initialization y.data=flow$year # dataset is fixed for anual ts. for(j in 2:nyrs){ y.dist = order(abs(y.p - y.data)) i1 = runif(1,0,1) %&gt;% c(.,W) %&gt;% rank(.) %&gt;% {y.dist[.[1]]} y.sim[j]=y.data[i1] # select y.data at given distance y.p = y.sim[j] # store previous year for next it. } # Get anual PDF year.pdf[isim,]=sm.density(y.sim,eval.points= yeval,display=&quot;none&quot;)$estimate # Calculate statistics ar.year.stat[isim,&quot;mean&quot;]=mean(y.sim) ar.year.stat[isim,&quot;max&quot;]=max(y.sim) ar.year.stat[isim,&quot;min&quot;]=min(y.sim) ar.year.stat[isim,&quot;stdev&quot;]=sd(y.sim) ar.year.stat[isim,&quot;skew&quot;]=skewness(y.sim) ar.year.stat[isim,&quot;cor&quot;]=cor(y.sim[-nyrs],y.sim[2:nyrs]) } # end isim loop We add the statistics from historical data in the first row of the tables # Compute statistics from the historical data. obsmean=1:12 obsstdev=1:12 obscor=1:12 obsskw=1:12 obsmax=1:12 obsmin=1:12 for(i in 1:12){ obsmax[i]=max(flow[,i]) obsmin[i]=min(flow[,i]) obsmean[i]=mean(flow[,i]) obsstdev[i]=sd(flow[,i]) obsskw[i]=skewness(flow[,i]) } obscor[1]= cor(flow[-nyrs,12], flow[2:nyrs,1]) for(i in 2:12){ i1=i-1 obscor[i]=cor(flow[,i], flow[,i1]) } # bind the stats of the historic data at the top.. armean=rbind(obsmean,armean) arstdev=rbind(obsstdev,arstdev) arskw=rbind(obsskw,arskw) arcor=rbind(obscor,arcor) armax=rbind(obsmax,armax) armin=rbind(obsmin,armin) # anual flow binding year.stat=c(mean(flow$year),sd(flow$year),min(flow$year), max(flow$year),skewness(flow$year), cor(flow$year[-nyrs],flow$year[2:nyrs])) ar.year.stat = rbind(year.stat,ar.year.stat) 2.2 Plot statistics from simulations Create boxplots of annual and monthly, mean, variance, skew, lag-1 correlation, minimum, maximum and PDFs of May and annual flows. Comment on what you observe and also on why some of the monthly statistics are not captured. # function to plot boxplots with the structure: hist. in first row plot.bp = function(matrix,name){ xmeans=as.matrix(matrix) n=length(xmeans[,1]) xmeans1=as.matrix(xmeans[2:n,]) #the first row is the original data xs=1:12 zz=boxplot(split(xmeans1,col(xmeans1)), plot=F, cex=1.0) zz$names=rep(&quot;&quot;,length(zz$names)) z1=bxp(zz,ylim=range(xmeans),xlab=&quot;&quot;,ylab=&quot;&quot;,cex=1.00) points(z1,xmeans[1,],pch=16, col=&quot;red&quot;) lines(z1,xmeans[1,],pch=16, col=&quot;gray&quot;) title(main=name) } The plots for the statistics of the simulated time series (shown as boxplots) vs. the historical data (shown as points and lines) are reproduced below: par(mfrow=c(3,2)) plot.bp(armean,&quot;Mean&quot;) plot.bp(arstdev,&quot;Standard Deviation&quot;) plot.bp(armin,&quot;Min&quot;) plot.bp(armax,&quot;Max&quot;) plot.bp(arskw,&quot;Skews&quot;) plot.bp(arcor,&quot;Lag-1 correlation&quot;) All the statistics are well fitted with respect the historical data. The anual statistics are similarly represented below: par(mfrow=c(2,3)) plot.bp(ar.year.stat[,&quot;mean&quot;],&quot;Mean&quot;) plot.bp(ar.year.stat[,&quot;stdev&quot;],&quot;Standard Deviation&quot;) plot.bp(ar.year.stat[,&quot;min&quot;],&quot;Min&quot;) plot.bp(ar.year.stat[,&quot;max&quot;],&quot;Max&quot;) plot.bp(ar.year.stat[,&quot;skew&quot;],&quot;Skews&quot;) plot.bp(ar.year.stat[,&quot;cor&quot;],&quot;Lag-1 correlation&quot;) For the anual flow, this is no longer true. Some statistics are not poorly fitted. xdensityorig = flow$may %&gt;% sm.density(.,eval.points=xeval,display=&quot;none&quot;) %&gt;% .$estimate plot.pdf = function(eval,histPDF,simPDF){ xeval = eval plot(xeval,histPDF,pch=&quot;.&quot;,col=&quot;red&quot;,ylim=range(simPDF,histPDF), xlab=&quot;&quot;,ylab = &quot;&quot;) for(i in 1:nsim)lines(xeval,simPDF[i,],col=&#39;lightgrey&#39;,lty=3) lines(xeval,histPDF,lwd=3,col=&quot;red&quot;) title(main=&quot;Historical vs. simulated PDF&quot;) } plot.pdf(xeval,xdensityorig,simpdf) The simulated PDF shows the bimodal behavior, and does not respond to a probabilistic distribution, as it happened to parametric AR. year.density = flow$year %&gt;% sm.density(.,eval.points=yeval, display=&quot;none&quot;) %&gt;% .$estimate plot.pdf(yeval,year.density,year.pdf) Many results are repeated (only 111 values for 100 simulations of 111 time series), creating a crisp PDF. Not realistic. 2.3 Advantages/disadvantages with a nonparametric approach. When the dataset is large enough, the fitted PDF and statistics over perform the parametric modelling. When the opposite occurs, many values are repeatedly picked, resulting in biased PDF and statistics. "],
["multivariate-simulation-copulas.html", "Exercise 3 Multivariate Simulation - Copulas 3.1 Fit a Copula to the joint CDF (copula by pairs) 3.2 Simulate from the Copula and invert 3.3 Boxplot the statistics listed in problem 1 and compare with the results from the previous methods 3.4 12 months copula", " Exercise 3 Multivariate Simulation - Copulas Another approach to simulating the monthly streamflow is using Copulas. Fit appropriate marginal PDFs for each monthly streamflow 3.0.1 Data import and setup # Load libraries libr=c(&quot;magrittr&quot;,&quot;sm&quot;,&quot;MASS&quot;,&quot;ks&quot;,&quot;copula&quot;,&quot;moments&quot;) options(warn=1) suppressPackageStartupMessages(lapply(libr, require, character.only = TRUE)) # import and set up flow data flow = read.table( &quot;http://civil.colorado.edu/~balajir/CVEN6833/HWs/HW-3-2018/LeesFerry-monflows-1906-2016.txt&quot;) flow = flow[,2:13] %&gt;% `rownames&lt;-`(flow[,1]) %&gt;% setNames(.,c(&quot;jan&quot;,&quot;feb&quot;,&quot;mar&quot;,&quot;apr&quot;,&quot;may&quot;,&quot;jun&quot;, &quot;jul&quot;,&quot;aug&quot;,&quot;sep&quot;,&quot;oct&quot;,&quot;nov&quot;,&quot;dec&quot;)) %&gt;% {./10^6} # convert AF to MAF head(flow,n=1L) # show values ## jan feb mar apr may jun jul aug ## 1906 0.244314 0.292534 0.678174 1.20464 3.635101 5.014167 2.95046 1.605086 ## sep oct nov dec ## 1906 1.503159 0.739807 0.503006 0.353312 tail(flow,n=1L) ## jan feb mar apr may jun jul ## 2016 0.360703 0.448837 0.67914 1.099567 2.967581 3.910287 1.342044 ## aug sep oct nov dec ## 2016 0.609946 0.485507 0.546633 0.426289 0.345163 3.1 Fit a Copula to the joint CDF (copula by pairs) As a first approach, we will use copula by pairwise comparison (lag-1 model). # Function to fit copula estimator for a pair of time series fit2copula = function(data1,data2){ u = pobs(cbind(data1,data2)) fc=normalCopula(dim=2,disp=&#39;un&#39;) fnc = fitCopula(fc,u) return(fnc) } # Function to generate simulated values from fitted copula sim.copula = function(fnc,data2,N){ cop.sim = rCopula(N,normalCopula(fnc,dim=2,dispstr=&#39;un&#39;)) data.sim = quantile(data2,cop.sim[,2]) # quan.sim = qkde(cop.sim[,2],fhat) return(data.sim) # return(quan.sim) } 3.2 Simulate from the Copula and invert # Simulation paramenters nsim=250 N=length(flow[,1]) # years armean=matrix(0,nsim,12) #matrices to store the statistics arstdev=matrix(0,nsim,12) arcor=matrix(0,nsim,12) arskw=matrix(0,nsim,12) armax=matrix(0,nsim,12) armin=matrix(0,nsim,12) xeval = matrix(0,nrow = 100,ncol = 12) # Points where the PDF is evaluated for(i in 1:12){ xeval[,i]=seq(min(flow[,i])-0.25*sd(flow[,i]), max(flow[,i])+0.25*sd(flow[,i]),length=100) } simpdf = array(0,c(100,12,nsim)) # Array to store simulated PDF xsim = array(0,c(N,12,nsim)) # Array to store simulated val # Simulation initialization parameters ncop = 12 # generate pairs of data to use in copula data1 = data2 = array(NA,c(N,1,ncop)) # data 1 / 2: prev / current month fnc = 1:ncop # fitted normal copulas for every pair for(icop in 1:ncop){ if(icop==1){ data1[-N,1,icop] = flow$dec[-N] # dec of previous year data2[-N,1,icop] = flow$jan[2:N] # jan of next year fnc[icop] = fit2copula(data1[-N,,icop],data2[-N,,icop])@estimate }else{ data1[,1,icop] = flow[,icop-1] # as given data2[,1,icop] = flow[,icop] # as given fnc[icop] = fit2copula(data1[,,icop],data2[,,icop])@estimate } } for(isim in 1:nsim){ # simulation loop for(j in 1:12){ # get each month as copula with previous month if(j==1){ # dec-jan copula xsim[,j,isim]=sim.copula(fnc[j],data2[-N,,j],N) # remove NA year 111 }else{ xsim[,j,isim]=sim.copula(fnc[j],data2[,,j],N) # simulated values } simpdf[,j,isim]=sm.density(xsim[,j,isim], # simulated PDF eval.points=xeval[,j],display=&quot;none&quot;)$estimate # fill statistics armean[isim,j]=mean(xsim[,j,isim]) armax[isim,j]=max(xsim[,j,isim]) armin[isim,j]=min(xsim[,j,isim]) arstdev[isim,j]=sd(xsim[,j,isim]) arskw[isim,j]=skewness(xsim[,j,isim]) if(j&gt;2)arcor[isim,j]=cor(xsim[,j,isim],xsim[,j-1,isim]) } arcor[isim,1]=cor(xsim[-N,12,isim],xsim[2:N,1,isim]) # print(isim) } We now add the statistics from the original dataset and bind it with the simulation st. # Compute statistics from the historical data. obsmean=1:12 obsstdev=1:12 obscor=1:12 obsskw=1:12 obsmax=1:12 obsmin=1:12 for(i in 1:12){ obsmax[i]=max(flow[,i]) obsmin[i]=min(flow[,i]) obsmean[i]=mean(flow[,i]) obsstdev[i]=sd(flow[,i]) obsskw[i]=skewness(flow[,i]) } obscor[1]= cor(flow[-N,12], flow[2:N,1]) for(i in 2:12){ obscor[i]=cor(flow[,i], flow[,i-1]) } # bind the stats of the historic data at the top.. armean=rbind(obsmean,armean) arstdev=rbind(obsstdev,arstdev) arskw=rbind(obsskw,arskw) arcor=rbind(obscor,arcor) armax=rbind(obsmax,armax) armin=rbind(obsmin,armin) 3.3 Boxplot the statistics listed in problem 1 and compare with the results from the previous methods We repeat the code from problem 1 to do so. # function to plot boxplots with the structure: hist. in first row plot.bp = function(matrix,name){ xmeans=as.matrix(matrix) n=length(xmeans[,1]) xmeans1=as.matrix(xmeans[2:n,]) #the first row is the original data xs=1:12 zz=boxplot(split(xmeans1,col(xmeans1)), plot=F, cex=1.0) zz$names=rep(&quot;&quot;,length(zz$names)) z1=bxp(zz,ylim=range(xmeans),xlab=&quot;&quot;,ylab=&quot;&quot;,cex=1.00) points(z1,xmeans[1,],pch=16, col=&quot;red&quot;) lines(z1,xmeans[1,],pch=16, col=&quot;gray&quot;) title(main=name) } par(mfrow=c(3,2)) plot.bp(armean,&quot;Mean&quot;) plot.bp(arstdev,&quot;Standard Deviation&quot;) plot.bp(armin,&quot;Min&quot;) plot.bp(armax,&quot;Max&quot;) plot.bp(arskw,&quot;Skews&quot;) plot.bp(arcor,&quot;Lag-1 correlation&quot;) As observed, mean, st.dev, max and skews are well fit. Min values are overestimated, and lag-1 is not captured. In general, the statics are better than the ones using autoregressive models. xdensityorig = matrix (0,nrow = 100,ncol = 12) #initialize original PDF for(j in 1:12){ # obtain original PDF xdensityorig[,j] = flow[,j] %&gt;% sm.density(.,eval.points=xeval[,j],display=&quot;none&quot;) %&gt;% .$estimate } plot.pdf = function(eval,histPDF,simPDF,title){ xeval = eval plot(xeval,histPDF,pch=&quot;.&quot;,col=&quot;red&quot;,ylim=range(simPDF,histPDF), xlab=&quot;&quot;,ylab = &quot;&quot;) for(i in 1:nsim)lines(xeval,simPDF[,i],col=&#39;lightgrey&#39;,lty=3) lines(xeval,histPDF,lwd=3,col=&quot;red&quot;) title(main=title) } par(mfrow=c(4,3)) par(mar=c(2,2,2,2)) for(j in 1:12){ plot.pdf(xeval[,j],xdensityorig[,j],simpdf[,j,],colnames(flow[j])) } The simulated PDF do not respond to a probabilistic distribution, sitting closely around the empirical PDF. The result is more precise that the one using autoregressive models. 3.4 12 months copula Now we will use the full copula between all months. u = pobs(flow) # convert to pseudo-observations fc12=normalCopula(dim=12,disp=&#39;un&#39;) # normal copula (66) fnc12 = fitCopula(fc12,u) # estimate from copula dat_sim = array(0,c(N,12,nsim)) cop_pdf = array(0,c(100,12,nsim)) carmean=matrix(0,nsim,12) #matrices to store the statistics carstdev=matrix(0,nsim,12) carcor=matrix(0,nsim,12) carskw=matrix(0,nsim,12) carmax=matrix(0,nsim,12) carmin=matrix(0,nsim,12) for(isim in 1:nsim){ cop_sim = rCopula(N,normalCopula(fnc12@estimate,dim=12,dispstr=&#39;un&#39;)) for(j in 1:12){ dat_sim[,j,isim] = quantile(flow[,j],cop_sim[,j]) cop_pdf[,j,isim]=sm.density(dat_sim[,j,isim], # simulated PDF eval.points=xeval[,j],display=&quot;none&quot;)$estimate # fill statistics carmean[isim,j]=mean(dat_sim[,j,isim]) carmax[isim,j]=max(dat_sim[,j,isim]) carmin[isim,j]=min(dat_sim[,j,isim]) carstdev[isim,j]=sd(dat_sim[,j,isim]) carskw[isim,j]=skewness(dat_sim[,j,isim]) if(j&gt;2)carcor[isim,j]=cor(dat_sim[,j,isim],dat_sim[,j-1,isim]) } carcor[isim,1]=cor(dat_sim[-N,12,isim],dat_sim[2:N,1,isim]) # print(isim) } # bind the stats of the historic data at the top.. carmean=rbind(obsmean,carmean) carstdev=rbind(obsstdev,carstdev) carskw=rbind(obsskw,carskw) carcor=rbind(obscor,carcor) carmax=rbind(obsmax,carmax) carmin=rbind(obsmin,carmin) par(mfrow=c(3,2)) plot.bp(carmean,&quot;Mean&quot;) plot.bp(carstdev,&quot;Standard Deviation&quot;) plot.bp(carmin,&quot;Min&quot;) plot.bp(carmax,&quot;Max&quot;) plot.bp(carskw,&quot;Skews&quot;) plot.bp(carcor,&quot;Lag-1 correlation&quot;) The results are at least as good as the copula by pairs. Also, the lag-1 correlation is captured (with the exception of jan and feb). par(mfrow=c(4,3)) par(mar=c(2,2,2,2)) for(j in 1:12){ plot.pdf(xeval[,j],xdensityorig[,j],cop_pdf[,j,],colnames(flow[j])) } The simulated PDF are also really close to the empirical PDF, being able to mimic the multimodal behavior of February, May, July, etc. "],
["non-stationary-time-series-hidden-markov-model.html", "Exercise 4 Non stationary time series - Hidden Markov Model 4.1 Fit a best HMM for the May Lees Ferry streamflow 4.2 Generate 250 simulations from the fitted HMM 4.3 Boxplot the resulting statistics 4.4 Fit a GLM for the state series 4.5 Use the state GLM to simulate flows from the component distribution", " Exercise 4 Non stationary time series - Hidden Markov Model Another way to simulate a time series is using Hidden Markov Model (Markov Chain + resampling). First, we load the libraries, streamflow data, and functions via R markdown (attached at the end of the document). # libraries libr=c(&quot;HiddenMarkov&quot;,&quot;ggplot2&quot;,&quot;data.table&quot;,&quot;ggthemes&quot;, &quot;magrittr&quot;,&quot;sm&quot;,&quot;moments&quot;,&quot;MASS&quot;,&quot;leaps&quot;) options(warn=1) suppressPackageStartupMessages(lapply(libr, require, character.only = TRUE)) ## Load flow data flow = read.table( &quot;http://civil.colorado.edu/~balajir/CVEN6833/HWs/HW-3-2018/LeesFerry-monflows-1906-2016.txt&quot;) flow = flow[,2:13] %&gt;% `rownames&lt;-`(flow[,1]) %&gt;% setNames(.,c(&quot;jan&quot;,&quot;feb&quot;,&quot;mar&quot;,&quot;apr&quot;,&quot;may&quot;,&quot;jun&quot;, &quot;jul&quot;,&quot;aug&quot;,&quot;sep&quot;,&quot;oct&quot;,&quot;nov&quot;,&quot;dec&quot;)) %&gt;% {./10^6} # convert AF to MAF x = flow$may ## select the May month flows 4.1 Fit a best HMM for the May Lees Ferry streamflow The code below fits HMM models of orders 2 through 6 and calculates the AIC for each. The best order is the one with the least value of AIC. ## Fit HMM models of orders 2 through 6. Obtain the AIC for each ## Best order is the one with the least value of AIC. family &lt;- &quot;gamma&quot; # underlying distribution for hmm discrete &lt;- FALSE aic1=c() for(imodel in 2:6){ m &lt;- imodel #model order to fit stationary &lt;- F # use a stationary distribution of mixtures # different initial condition types when family == &quot;norm&quot; ic &lt;- &quot;same.sd&quot;#c(&quot;same.sd&quot;,&quot;same.both&quot;,&quot;both.diff&quot;) fd.name &lt;- ifelse(family == &quot;norm&quot;, &quot;normal&quot;, family) Pi &lt;- Pi_init(m) # T.P.M. delta &lt;- delta_init(m) pars &lt;- get.named.parlist(x, m, fd.name, lower=.0, ic)#,start=list(shape1=2,shape2=2)) # set up the model hmm &lt;- dthmm(x, Pi=Pi, delta=delta, family, pars, nonstat=!stationary, discrete = discrete) sink(&quot;p.4.hmm.fit&quot;) if(imodel &lt; 2){ hmm &lt;- BaumWelch(hmm, bwcontrol(maxiter = 1000, posdiff=TRUE,converge = expression(diff &gt; tol))) } else { hmm &lt;- BaumWelch(hmm, bwcontrol(maxiter = 1000, tol = 1e-08)) } sink() # get the hidden states from the fitted model # Global decoding. To get the probability of being in a state: hmm$u decoding &lt;- Viterbi(hmm) # get AIC aic &lt;- AIC(hmm) aic1=c(aic1,aic) } We select the HMM with the lowest AIC. In this case, this happens to be of order 2. We rerun the HMM for best order (m=2) and generate the state sequence (decoding) resulting from it. The model summary is also attached. ## Get the best order bestorder = order(aic1)[1] +1 ## Fit the model for this best order m &lt;- bestorder #model order to fit stationary &lt;- F # use a stationary distribution of mixtures # different initial condition types when family == &quot;norm&quot; ic &lt;- &quot;same.sd&quot;#c(&quot;same.sd&quot;,&quot;same.both&quot;,&quot;both.diff&quot;) fd.name &lt;- ifelse(family == &quot;norm&quot;, &quot;normal&quot;, family) Pi &lt;- Pi_init(m) # T.P.M. delta &lt;- delta_init(m) pars &lt;- get.named.parlist(x, m, fd.name, lower=.0, ic)#,start=list(shape1=2,shape2=2)) # set up the model hmm &lt;- dthmm(x, Pi=Pi, delta=delta, family, pars, nonstat=!stationary, discrete = discrete) sink(&quot;p.4.best.hmm&quot;) hmm &lt;- BaumWelch(hmm, bwcontrol(maxiter = 1000, tol = 1e-08)) sink() # end hidding output decoding &lt;- Viterbi(hmm) print(summary(hmm)) ## $delta ## [1] 1.000000e+00 6.556793e-105 ## ## $Pi ## [,1] [,2] ## [1,] 0.7046162 0.2953838 ## [2,] 0.3414249 0.6585751 ## ## $nonstat ## [1] TRUE ## ## $distn ## [1] &quot;gamma&quot; ## ## $pm ## $pm$rate ## [1] 4.062193 2.974823 ## ## $pm$shape ## [1] 15.203074 6.992024 ## ## ## $discrete ## [1] FALSE ## ## $n ## [1] 111 cat(&#39;Model order:&#39;,m,&#39;\\n&#39;) ## Model order: 2 p &lt;- ggplot_stationary_hmm(hmm,.5) print(p) ## Warning: Removed 1 rows containing missing values (geom_bar). state.1=ifelse(decoding==1,2.5,NA) state.2=ifelse(decoding==2,0.5,NA) plot(flow$may-mean(flow$may)+1.5,type=&quot;l&quot;,ylab=&quot;&quot;, main=&quot;Modified May flows and states (supperposed)&quot;) points(state.2,col=&#39;blue&#39;) points(state.1,col=&#39;red&#39;) legend(&quot;topright&quot;,legend=c(&quot;state 1&quot;,&quot;state 2&quot;),col=c(&quot;red&quot;,&quot;blue&quot;),pch=1) 4.2 Generate 250 simulations from the fitted HMM This involves generating the state sequence from the transition probability matrix and resampling flows from the corresponding component distribution. # Now simulate # First simulate a sequence of states from the TPM # simulate from the transition probability N = length(x) nsim = 250 nprob = length(decoding[decoding == 1])/N delta1=c(nprob,1-nprob) #stationary probability zsim = mchain(NULL,hmm$Pi,delta=delta1) may.sim = matrix(0,nrow=nsim,ncol=N) # Points where May PDF is evaluated xeval=seq(min(flow$may)-0.25*sd(flow$may), max(flow$may)+0.25*sd(flow$may),length=100) sim.pdf=matrix(0,nrow=nsim,ncol=100) # Array to store May simulated PDF may.stat=matrix(NA,ncol = 6,nrow = nsim) # year statistics colnames(may.stat) = c(&quot;mean&quot;,&quot;stdev&quot;,&quot;min&quot;,&quot;max&quot;,&quot;skew&quot;,&quot;cor&quot;) for(isim in 1:nsim){ zsim = simulate(zsim,nsim=N) ## now simulate the flows from the corresponding PDF flowsim = c() for(i in 1:N){ if(zsim$mc[i] == 1)xx=rgamma(1,shape=hmm$pm$shape[1], scale=1/hmm$pm$rate[1]) if(zsim$mc[i] == 2)xx=rgamma(1,shape=hmm$pm$shape[2], scale=1/hmm$pm$rate[2]) flowsim=c(flowsim,xx) } may.sim[isim,]=flowsim sim.pdf[isim,]=sm.density(flowsim,eval.points=xeval, display=&quot;none&quot;)$estimate # fill statistics may.stat[isim,&quot;mean&quot;]=mean(flowsim) may.stat[isim,&quot;max&quot;]=max(flowsim) may.stat[isim,&quot;min&quot;]=min(flowsim) may.stat[isim,&quot;stdev&quot;]=sd(flowsim) may.stat[isim,&quot;skew&quot;]=skewness(flowsim) may.stat[isim,&quot;cor&quot;]=cor(flowsim[-N],flowsim[2:N]) } The statistics from the historical data is added in the first row of the statistics matrix. # Compute statistics from the historical data. obs=1:6 obs[1]=mean(flow$may) obs[2]=sd(flow$may) obs[3]=min(flow$may) obs[4]=max(flow$may) obs[5]=skewness(flow$may) obs[6]=cor(flow$may[-N],flow$may[2:N]) # bind the stats of the historic data at the top.. may.stat=rbind(obs,may.stat) 4.3 Boxplot the resulting statistics We include the mean, variance, skew, lag-1 correlation, minimum, maximum and PDF from the simulations with the corresponding values from the historical data plotted on them. # function to plot boxplots with the structure: hist. in first row plot.bp = function(matrix,name){ xmeans=as.matrix(matrix) n=length(xmeans[,1]) xmeans1=as.matrix(xmeans[2:n,]) #the first row is the original data xs=1:12 zz=boxplot(split(xmeans1,col(xmeans1)), plot=F, cex=1.0) zz$names=rep(&quot;&quot;,length(zz$names)) z1=bxp(zz,ylim=range(xmeans),xlab=&quot;&quot;,ylab=&quot;&quot;,cex=1.00) points(z1,xmeans[1,],pch=16, col=&quot;red&quot;) lines(z1,xmeans[1,],pch=16, col=&quot;gray&quot;) title(main=name) } par(mfrow=c(2,3)) plot.bp(may.stat[,&quot;mean&quot;],&quot;Mean&quot;) plot.bp(may.stat[,&quot;stdev&quot;],&quot;Standard Deviation&quot;) plot.bp(may.stat[,&quot;min&quot;],&quot;Min&quot;) plot.bp(may.stat[,&quot;max&quot;],&quot;Max&quot;) plot.bp(may.stat[,&quot;skew&quot;],&quot;Skews&quot;) plot.bp(may.stat[,&quot;cor&quot;],&quot;Lag-1 correlation&quot;) xdensityorig = flow$may %&gt;% sm.density(.,eval.points=xeval,display=&quot;none&quot;) %&gt;% .$estimate plot.pdf = function(eval,histPDF,simPDF){ xeval = eval plot(xeval,histPDF,pch=&quot;.&quot;,col=&quot;red&quot;,ylim=range(simPDF,histPDF), xlab=&quot;&quot;,ylab = &quot;&quot;) for(i in 1:nsim)lines(xeval,simPDF[i,],col=&#39;lightgrey&#39;,lty=3) lines(xeval,histPDF,lwd=3,col=&quot;red&quot;) title(main=&quot;Historical vs. simulated PDF&quot;) } plot.pdf(xeval,xdensityorig,sim.pdf) The model is able to represent accurately all statistics, including lag-1 correlation. 4.4 Fit a GLM for the state series Instead of simulating the streamflow from a “static” state sequence simulation, the goal is to simulate the state as a prediction from a best logistic GLM. The uncertainty is added choosing random values and comparing them to the initial probability (p1 / p2) of being in a particular state. enso=read.table(&quot;C:/Users/alexb/Google Drive/CVEN 6833 ADAT/zz Homeworks/HW3/enso.txt&quot;) amo=read.table(&quot;C:/Users/alexb/Google Drive/CVEN 6833 ADAT/zz Homeworks/HW3/amo.txt&quot;) pdo=read.table(&quot;C:/Users/alexb/Google Drive/CVEN 6833 ADAT/zz Homeworks/HW3/pdo.txt&quot;) X = data.frame(decoding[-N]-1,enso[-N,],amo[-N,],pdo[-N,]) colnames(X) = c(&quot;ts1&quot;,&quot;enso&quot;,&quot;amo&quot;,&quot;pdo&quot;) Y = decoding[-1]-1 links = c(&quot;logit&quot;, &quot;probit&quot;, &quot;cauchit&quot;,&quot;log&quot;,&quot;cloglog&quot;) # potential links comb=leaps(X,Y, nbest=40,method=&quot;adjr2&quot;)$which # all combinations of cov. aic &lt;- matrix(1e6,ncol=length(links),nrow = length(comb[,1])) colnames(aic) = links[1:length(links)] for(k in 1:length(comb[,1])){ # try every link f. with every comb. xx = X[,comb[k,]] %&gt;% as.data.frame(.) for(i in 1:length(links)){ zz=try(glm(Y ~ ., data=xx, family = binomial(link=links[i]), maxit=500),silent=TRUE) if(class(zz)[1]!=&quot;try-error&quot;)aic[k,i]=zz$aic[1] } } head(aic) ## logit probit cauchit log cloglog ## [1,] 125.3869 125.3869 125.3869 125.3869 125.3869 ## [2,] 146.9211 146.8480 147.3203 147.6516 147.2601 ## [3,] 148.3173 148.4183 147.6240 148.3154 148.2197 ## [4,] 151.5616 151.5576 151.5843 151.5754 151.5692 ## [5,] 124.8050 124.6801 125.4507 1000000.0000 125.1181 ## [6,] 125.5837 125.6196 125.6204 1000000.0000 125.7381 index = which(aic == min(aic), arr.ind = TRUE) # select min. AIC print( sprintf(&quot;Choosing the GLM which minimizes AIC for binomial family: %s link function and %s covariates&quot;,links[index[,&quot;col&quot;]], paste(colnames(X)[comb[index[,&quot;row&quot;],]],collapse = &#39;, &#39;))) ## [1] &quot;Choosing the GLM which minimizes AIC for binomial family: probit link function and ts1, amo covariates&quot; state.glm = glm(Y ~ ., data=X[,comb[index[,&quot;row&quot;],]], family = binomial(link=links[index[,&quot;col&quot;]])) summary(state.glm) # Model selected ## ## Call: ## glm(formula = Y ~ ., family = binomial(link = links[index[, &quot;col&quot;]]), ## data = X[, comb[index[, &quot;row&quot;], ]]) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7872 -0.7333 -0.5345 0.8427 1.9937 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.8180 0.1756 -4.658 3.19e-06 *** ## ts1 1.2820 0.2683 4.779 1.76e-06 *** ## amo 0.9847 0.5931 1.660 0.0969 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 148.06 on 109 degrees of freedom ## Residual deviance: 118.68 on 107 degrees of freedom ## AIC: 124.68 ## ## Number of Fisher Scoring iterations: 4 ENSO and PDO covariates resulted to be insignificant for the logistic regression, even if several laggings from the original data have been tried. 4.5 Use the state GLM to simulate flows from the component distribution With the fitted values from the logistic regression, we calculate first the probabilities (p1, p2) of each state. These are used to generate a random initial state and simulate the rest of the state sequence. Depending on the simulated sequence, a random gamma value is taken from either state 1 or state 2, creating a new streamflow time series, with the same length as the original data. The simulation is repeated 250 to generate statistics and ensemble PDF. X.new = X[,comb[index[,&quot;row&quot;],]] p1 = 1 - sum(round(state.glm$fitted.values))/length(state.glm$fitted.values) state = 1:N # init. state state.p1 = 1:nsim # store p1 for debugging may.sim.glm = matrix(0,nrow=nsim,ncol=N) sim.pdf.glm = matrix(0,nrow=nsim,ncol=100) # Array to store May simulated PDF may.stat.glm=matrix(NA,ncol = 6,nrow = nsim) # statistics colnames(may.stat.glm) = c(&quot;mean&quot;,&quot;stdev&quot;,&quot;min&quot;,&quot;max&quot;,&quot;skew&quot;,&quot;cor&quot;) # simulation for(isim in 1:nsim){ # for(j in 2:N)state[j]=round(predict(state.glm,newdata = data.frame(ts1=state[j-1],amo = amo[j-1,1]))) for(j in 1:N){ if(j==1){ state[1]=ifelse(runif(1)&lt;p1,1,2) }else{ state[j]=ifelse(state.glm$fitted.values[j-1]&lt;runif(1),1,2) } if(state[j]==1){ may.sim.glm[isim,j]=rgamma(1,shape = hmm$pm$shape[1],hmm$pm$rate[1]) }else{ may.sim.glm[isim,j]=rgamma(1,shape = hmm$pm$shape[2],hmm$pm$rate[2]) } } state.p1[isim] = 1 - sum((state-1))/length(state) sim.pdf.glm[isim,]=sm.density(may.sim.glm[isim,], eval.points=xeval,display=&quot;none&quot;)$estimate # fill statistics may.stat.glm[isim,&quot;mean&quot;]=mean(may.sim.glm[isim,]) may.stat.glm[isim,&quot;max&quot;]=max(may.sim.glm[isim,]) may.stat.glm[isim,&quot;min&quot;]=min(may.sim.glm[isim,]) may.stat.glm[isim,&quot;stdev&quot;]=sd(may.sim.glm[isim,]) may.stat.glm[isim,&quot;skew&quot;]=skewness(may.sim.glm[isim,]) may.stat.glm[isim,&quot;cor&quot;]=cor(may.sim.glm[isim,-N],may.sim.glm[isim,2:N]) } # bind the stats of the historic data at the top.. may.stat.glm=rbind(obs,may.stat.glm) par(mfrow=c(2,3)) plot.bp(may.stat.glm[,&quot;mean&quot;],&quot;Mean&quot;) plot.bp(may.stat.glm[,&quot;stdev&quot;],&quot;Standard Deviation&quot;) plot.bp(may.stat.glm[,&quot;min&quot;],&quot;Min&quot;) plot.bp(may.stat.glm[,&quot;max&quot;],&quot;Max&quot;) plot.bp(may.stat.glm[,&quot;skew&quot;],&quot;Skews&quot;) plot.bp(may.stat.glm[,&quot;cor&quot;],&quot;Lag-1 correlation&quot;) plot.pdf(xeval,xdensityorig,sim.pdf.glm) Functions from lib.R: # source functions # takend from &quot;http://civil.colorado.edu/~balajir/CVEN6833/R-sessions/session3/files-4HW3/lib.R&quot; Pi_init &lt;- function(n,type=&#39;uniform&#39;){ matrix(rep(1/n,n^2),n)} delta_init &lt;- function(n, type=&#39;uniform&#39;){ d &lt;- rnorm(n)^2 d/sum(d)} ntile.ts &lt;- function(x, n, limit.type = &#39;prob&#39;, tie = 1, altobs = NULL ){ # returns an integer vector corresponding to n states broken by equal # probability or equal distance # limit &lt;- if(limit.type == &#39;prob&#39;) quantile(x,seq(0,1,1/n)) else if(limit.type == &#39;equal&#39;) seq(min(x),max(x),by=diff(range(x))/n) if(!is.null(altobs)) limit &lt;- quantile(altobs,seq(0,1,1/n)) b &lt;- integer(length(x)) for(i in 1:(n+1)){ filter &lt;- if(tie == 1) x &gt;= limit[i] &amp; x &lt;= limit[i+1] else x &gt; limit[i] &amp; x &lt;= limit[i+1] #only need to set the 1&#39;s because b is already 0&#39;s b[filter] &lt;- as.integer(i-1) } if(class(x) == &#39;ts&#39;) return(ts(b,start=start(x),end=end(x))) else return(b) } get.named.parlist &lt;- function(x,m,dist,ic,...){ require(MASS) fit &lt;- fitdistr(x,dist,...) np &lt;- length(fit$estimate) pars &lt;- vector(&#39;list&#39;,np) names(pars) &lt;- names(fit$estimate) init &lt;- lapply(fit$estimate,max) names(init) &lt;- names(fit$estimate) for(j in 1:m){ #print(j) #browser() #browser() this.fit &lt;- fitdistr(x[ntile.ts(x,m) == (j-1)],dist,init,...) #for(k in 1:np) # pars[[k]][j] &lt;- this.fit$estimate[k] for(k in 1:np) pars[[k]][j] &lt;- fit$estimate[k] if(dist == &#39;normal&#39;){ if(ic == &#39;same.both&#39;){ pars[[k]][j] &lt;- mean(x) pars[[k]][j] &lt;- sd(x) } else if( ic == &#39;same.sd&#39;){ pars[[k]][j] &lt;- mean(x[ntile.ts(x,m) == (j-1)]) pars[[k]][j] &lt;- sd(x) }else{ pars[[k]][j] &lt;- mean(x[ntile.ts(x,m) == (j-1)]) pars[[k]][j] &lt;- sd(x[ntile.ts(x,m) == (j-1)]) } } } pars } AIC.dthmm &lt;- function(x){ ## Return the Akaieke Information criterion value for a fitted discrete ## time hidden markov model from the HiddenMarkov package # Model order m &lt;- length(x$delta) # Log Liklihood value LL &lt;- x$LL # number of parameters p &lt;- m+m^2 # AIC -2*LL + 2*p } ggplot_stationary_hmm &lt;- function(x,binwidth=NULL,res=1000,cols=NULL,...){ m &lt;- length(x$delta) dens &lt;- matrix(0,nrow=m+1,ncol=res) r &lt;- extendrange(x$x,f=.05) xrange &lt;- seq(r[1],r[2],len=res) delta &lt;- statdist(x$Pi) if(is.null(binwidth)) binwidth &lt;- diff(range(x$x))/8 for(i in 1:m){ if(x$distn == &#39;gamma&#39;){ dens[i,] &lt;- delta[i]*dgamma(xrange,shape=x$pm$shape[i],rate=x$pm$rate[i]) }else if(x$distn == &#39;norm&#39;){ dens[i,] &lt;- delta[i]*dnorm(xrange,mean=x$pm$mean[i],sd=x$pm$sd[i]) }else{ stop(&#39;Distribution not supported&#39;) } dens[m+1,] &lt;- dens[m+1,] + dens[i,] } p &lt;- ggplot()+ geom_histogram(data=data.frame(x=as.vector(x$x)),aes(x=x,y=..density..), binwidth=binwidth,fill=&#39;white&#39;,color=&#39;black&#39;)+ theme_bw() dt &lt;- data.table(x=numeric(0),y=numeric(0), state=integer(0)) for(i in 1:m) dt &lt;- rbind(dt, data.table(x=xrange,y=dens[i,], state=i)) dt$state &lt;- factor(dt$state) p &lt;- p + geom_line(data=dt,aes(x=x,y=y,color=state)) + geom_line(data=data.frame(x=xrange,y=dens[m+1,]),aes(x=x,y=y),color=&#39;black&#39;,size=1) + scale_color_tableau() + scale_x_continuous(limits=r) p } statdist &lt;- function(tpm){ m &lt;- nrow(tpm) ones &lt;- rbind(rep(1,m)) I &lt;- diag(rep(1,m)) U &lt;- matrix(rep(1,m^2),m) as.vector(ones %*% solve(I - tpm + U)) } "],
["spectral-simulation.html", "Exercise 5 Spectral Simulation", " Exercise 5 Spectral Simulation Placeholder # Placeholder "],
["modeling-nonstationary-extreme-value-time-series.html", "Exercise 6 Modeling Nonstationary Extreme Value Time series 6.1 Fit a stationary GEV 6.2 Fit a best nonstationary GEV model 6.3 Repeat by varying scale and location 6.4 Plot the results", " Exercise 6 Modeling Nonstationary Extreme Value Time series Annual maximum flow on Clark Fork River, MT has a strong relationship with two large scale climate drivers – winter El Nino Southern Oscillation (ENSO) and winter Pacific Decadal Oscillation (PDO). # libraries libr=c(&quot;extRemes&quot;,&quot;tidyverse&quot;,&quot;reshape2&quot;) options(warn=-1) suppressPackageStartupMessages(lapply(libr, require, character.only = TRUE)) 6.1 Fit a stationary GEV Fit a stationary GEV to the annual maximum flow series – traditional approach. Estimate the 2-year, 50-year, 100-year and 1000-year return period. data=read.table( &quot;http://civil.colorado.edu/~balajir/CVEN6833/HWs/HW-3-2018/Cfork-enso-pdo.txt&quot;) colnames(data) = c(&quot;year&quot;,&quot;CFR&quot;,&quot;ENSO&quot;,&quot;PDO&quot;) rownames(data) = data$year data = data[,!colnames(data) %in% &quot;year&quot;] fit.gev.s &lt;- fevd(CFR,data) # fits a stationary GEV summary(fit.gev.s) # model info ## ## fevd(x = CFR, data = data) ## ## [1] &quot;Estimation Method used: MLE&quot; ## ## ## Negative Log-Likelihood Value: 86.67505 ## ## ## Estimated parameters: ## location scale shape ## -0.32384381 0.70877948 -0.01945511 ## ## Standard Error Estimates: ## location scale shape ## 0.09392425 0.06716585 0.08069709 ## ## Estimated parameter covariance matrix. ## location scale shape ## location 0.008821764 0.002204481 -0.002573214 ## scale 0.002204481 0.004511251 -0.001523174 ## shape -0.002573214 -0.001523174 0.006512021 ## ## AIC = 179.3501 ## ## BIC = 186.1381 rl.s = return.level(fit.gev.s,return.period = c(2,50,100,1000)) # return periods 6.2 Fit a best nonstationary GEV model Fit a best nonstationary GEV model varying just the location parameter and estimate the above four return periods. alt = c(&quot;~ ENSO&quot;,&quot;~ PDO&quot;,&quot;~ ENSO + PDO&quot;) # alternatives AIC.ns = BIC.ns = 1:3 # AIC, BIC values for(i in 1:3){ bns=fevd(CFR,data,location.fun = eval(parse(text = alt[i])), use.phi = TRUE) # location: ENSO AIC.ns[i]=summary(bns,silent=TRUE)$AIC BIC.ns[i]=summary(bns,silent=TRUE)$BIC } print(AIC.ns) ## [1] 163.5837 174.0022 164.2446 print(BIC.ns) ## [1] 172.6344 183.0529 175.5580 index = which(AIC.ns == min(AIC.ns),arr.ind = TRUE) print(sprintf(&quot;Best non-stationary GEV model: %s location&quot;, alt[index])) ## [1] &quot;Best non-stationary GEV model: ~ ENSO location&quot; fit.gev.ns = fevd(CFR,data,location.fun = eval(parse(text = alt[index[1]])), use.phi = TRUE) summary(fit.gev.ns) ## ## fevd(x = CFR, data = data, location.fun = eval(parse(text = alt[index[1]])), ## use.phi = TRUE) ## ## [1] &quot;Estimation Method used: MLE&quot; ## ## ## Negative Log-Likelihood Value: 77.79183 ## ## ## Estimated parameters: ## mu0 mu1 scale shape ## -0.24530588 0.36192879 0.62352993 -0.01653051 ## ## Standard Error Estimates: ## mu0 mu1 scale shape ## 0.08430145 0.08447899 0.06016464 0.08631796 ## ## Estimated parameter covariance matrix. ## mu0 mu1 scale shape ## mu0 0.007106734 0.0012043692 0.0018846668 -0.002714174 ## mu1 0.001204369 0.0071366989 0.0004828137 -0.001382696 ## scale 0.001884667 0.0004828137 0.0036197834 -0.001628543 ## shape -0.002714174 -0.0013826961 -0.0016285431 0.007450791 ## ## AIC = 163.5837 ## ## BIC = 172.6344 rl.ns = return.level(fit.gev.ns,return.period = c(2,50,100,1000)) rl.ns[c(1:3,69:71),] ## 2-year level 50-year level 100-year level 1000-year level ## 1930 0.03248117 2.160811 2.566626 3.874737 ## 1931 0.24746687 2.375797 2.781611 4.089723 ## 1932 0.06215933 2.190489 2.596304 3.904415 ## 1998 0.43060283 2.558933 2.964747 4.272859 ## 1999 -0.18178068 1.946549 2.352364 3.660475 ## 2000 -0.46770442 1.660626 2.066440 3.374551 6.3 Repeat by varying scale and location options(warn=-999) alt = c(&quot;~ ENSO&quot;,&quot;~ PDO&quot;,&quot;~ ENSO + PDO&quot;) # alternatives AIC.ns = BIC.ns = matrix(NA,nrow = 3,ncol = 3) # store AIC, BIC for all alt. for(i in 1:3){ for(j in 1:3){ bns=fevd(CFR,data,location.fun = eval(parse(text = alt[i])), scale.fun = eval(parse(text = alt[j])),use.phi = TRUE) AIC.ns[i,j] = summary(bns,silent=TRUE)$AIC BIC.ns[i,j] = summary(bns,silent=TRUE)$BIC } } print(AIC.ns) ## [,1] [,2] [,3] ## [1,] 163.7752 159.9197 161.8728 ## [2,] 174.2619 172.6655 174.4243 ## [3,] 164.3930 160.9743 162.8786 print(BIC.ns) ## [,1] [,2] [,3] ## [1,] 175.0886 171.2331 175.4489 ## [2,] 185.5753 183.9789 188.0004 ## [3,] 177.9690 174.5503 178.7174 index = which(AIC.ns == min(AIC.ns),arr.ind = TRUE) print(sprintf(&quot;Best non-stationary GEV model: %s location and %s scale&quot;, alt[index[1]],alt[index[2]])) ## [1] &quot;Best non-stationary GEV model: ~ ENSO location and ~ PDO scale&quot; fit.gev.ns.ls = fevd(CFR,data,location.fun = eval(parse(text = alt[index[1]])), scale.fun = eval(parse(text = alt[index[2]])),use.phi = TRUE) summary(fit.gev.ns.ls) ## ## fevd(x = CFR, data = data, location.fun = eval(parse(text = alt[index[1]])), ## scale.fun = eval(parse(text = alt[index[2]])), use.phi = TRUE) ## ## [1] &quot;Estimation Method used: MLE&quot; ## ## ## Negative Log-Likelihood Value: 74.95986 ## ## ## Estimated parameters: ## mu0 mu1 phi0 phi1 shape ## -2.693319e-01 3.602204e-01 1.663393e-01 -2.235114e-05 -6.554171e-02 ## ## Standard Error Estimates: ## mu0 mu1 phi0 phi1 shape ## 0.07822364 0.07563862 0.09533687 0.00000002 0.07896250 ## ## Estimated parameter covariance matrix. ## mu0 mu1 phi0 phi1 ## mu0 6.118937e-03 1.605458e-03 2.141504e-03 -3.040912e-14 ## mu1 1.605458e-03 5.721201e-03 4.574934e-06 -6.495898e-17 ## phi0 2.141504e-03 4.574934e-06 9.089118e-03 -1.290651e-13 ## phi1 -3.040912e-14 -6.495898e-17 -1.290651e-13 3.999998e-16 ## shape -2.059310e-03 -1.309432e-04 -2.805681e-03 3.983996e-14 ## shape ## mu0 -2.059310e-03 ## mu1 -1.309432e-04 ## phi0 -2.805681e-03 ## phi1 3.983996e-14 ## shape 6.235077e-03 ## ## AIC = 159.9197 ## ## BIC = 171.2331 rl.ns.ls = return.level(fit.gev.ns.ls,return.period = c(2,50,100,1000)) rl.ns.ls[c(1:3,69:71),] ## 2-year level 50-year level 100-year level 1000-year level ## 1930 0.06961405 2.530161 2.952235 4.217186 ## 1931 0.31996007 3.089952 3.565108 4.989143 ## 1932 0.05017785 2.094097 2.444704 3.495470 ## 1998 0.44409223 2.719489 3.109804 4.279570 ## 1999 -0.22040302 1.587085 1.897135 2.826353 ## 2000 -0.40116031 2.289504 2.751052 4.134305 6.4 Plot the results Include the annual maximum flow series, the stationary return periods and the nonstationary. series.stat = data.frame( year = rownames(data), observed = data$CFR, &quot;2.year&quot; = rep(rl.s[1],length.out = length(data[,1])), &quot;50.year&quot; = rep(rl.s[2],length.out = length(data[,1])), &quot;100.year&quot; = rep(rl.s[3],length.out = length(data[,1])), &quot;1000.year&quot; = rep(rl.s[4],length.out = length(data[,1])) ) series.non.stat = data.frame(year = rownames(data), observed = data$CFR,&quot;2.year&quot; = rl.ns[,1],&quot;50.year&quot; = rl.ns[,2], &quot;100.year&quot; = rl.ns[,3],&quot;1000.year&quot; = rl.ns[,4]) series.non.stat.ls = data.frame(year = rownames(data), observed = data$CFR,&quot;2.year&quot; = rl.ns.ls[,1],&quot;50.year&quot; = rl.ns.ls[,2], &quot;100.year&quot; = rl.ns.ls[,3],&quot;1000.year&quot; = rl.ns.ls[,4]) plot.series = function(series,title){ series = melt(series,id.vars = &quot;year&quot;,variable.name =&quot;series&quot;) ggplot(series, aes(x=as.numeric(as.character(year)),y=value,group=series))+ geom_line(aes(linetype=series,color=series))+ scale_colour_manual(&quot;&quot;, values = c(&quot;black&quot;, &quot;green&quot;, &quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;)) + scale_linetype_manual(&quot;&quot;,values=c(&quot;solid&quot;,&quot;longdash&quot;,&quot;longdash&quot;,&quot;longdash&quot;,&quot;longdash&quot;)) + labs(title = title, x = &quot;Year&quot;, y = &quot;Peak Flow at return level&quot;)+ theme(plot.title = element_text(hjust = 0.5),plot.margin=unit(c(1,9,1,1),&quot;cm&quot;),legend.position=c(1.25, .8)) } plot.series(series.stat,&quot;Historical Stationary Return Levels\\n&quot;) plot.series(series.non.stat,&quot;Historical Non-Stationary Return Levels (best location) \\n&quot;) plot.series(series.non.stat.ls,&quot;Historical Non-Stationary Return Levels (best location and scale) \\n&quot;) "],
["use-of-hidden-markov-models-for-forecasting.html", "Exercise 7 Use of Hidden Markov Models for forecasting 7.1 Fit a best HMM model for the spring season flows 7.2 Generate 250 simulations each of same length as the historical data 7.3 Fit a best GLM to the state sequence 7.4 For each year based on the predictors, obtain the probabilities of the states 7.5 Using these state probabilities, simulate flow from the corresponding state PDFs 7.6 Compute the median and compare with the observed flow. Compute RPSS at the terciles. 7.7 Make blind predictions from 2001 and on 7.8 Functions from lib.R", " Exercise 7 Use of Hidden Markov Models for forecasting For the spring season flow (Apr-Jun average) at Lees Ferry on the Colorado River, use HMM to develop a forecasting model, as follows: 7.1 Fit a best HMM model for the spring season flows First we load libraries, data, and functions as in exercise 4. set.seed(1) # allow repetition of results # libraries libr=c(&quot;HiddenMarkov&quot;,&quot;ggplot2&quot;,&quot;data.table&quot;,&quot;ggthemes&quot;, &quot;magrittr&quot;,&quot;sm&quot;,&quot;moments&quot;,&quot;MASS&quot;,&quot;leaps&quot;,&quot;verification&quot;) options(warn=-999) suppressPackageStartupMessages(lapply(libr, require, character.only = TRUE)) # Load flow data mflow = read.table( &quot;http://civil.colorado.edu/~balajir/CVEN6833/HWs/HW-3-2018/LeesFerry-monflows-1906-2016.txt&quot;) x = mflow[,2:13] %&gt;% `rownames&lt;-`(mflow[,1]) %&gt;% # Apr - Jun {rowMeans(.[,4:6])} %&gt;% {.*0.0004690502*0.001} # convert to 10^3 cms Finding a best HMM implies obtaining the best component (state) PDFs and the state sequence. ## Fit HMM models of orders 1 through 6. Obtain the AIC for each ## Best order is the one with the least value of AIC. family &lt;- &quot;gamma&quot; # underlying distribution for hmm discrete &lt;- FALSE aic1=c() for(imodel in 1:6){ m &lt;- imodel #model order to fit stationary &lt;- F # use a stationary distribution of mixtures # different initial condition types when family == &quot;norm&quot; ic &lt;- &quot;same.sd&quot;#c(&quot;same.sd&quot;,&quot;same.both&quot;,&quot;both.diff&quot;) fd.name &lt;- ifelse(family == &quot;norm&quot;, &quot;normal&quot;, family) Pi &lt;- Pi_init(m) # T.P.M. delta &lt;- delta_init(m) pars &lt;- get.named.parlist(x, m, fd.name, lower=.0, ic)#,start=list(shape1=2,shape2=2)) # set up the model hmm &lt;- dthmm(x, Pi=Pi, delta=delta, family, pars, nonstat=!stationary, discrete = discrete) sink(&quot;p.7.hmm.fit&quot;) if(imodel &lt; 2){ hmm &lt;- BaumWelch(hmm, bwcontrol(maxiter = 1000, posdiff=TRUE,converge = expression(diff &gt; tol))) } else { hmm &lt;- BaumWelch(hmm, bwcontrol(maxiter = 1000, tol = 1e-08)) } sink() # get the hidden states from the fitted model # Global decoding. To get the probability of being in a state: hmm$u decoding &lt;- Viterbi(hmm) # get AIC aic &lt;- AIC(hmm) aic1=c(aic1,aic) } print(aic1) ## [1] 133.1062 132.7230 139.4608 147.7127 158.6281 159.5437 We select the HMM with the lowest AIC. In this case, this happens to be of order 2. We rerun the HMM for best order (m=2) and generate the state sequence (decoding) resulting from it. ## Get the best order bestorder = order(aic1)[1] ## Fit the model for this best order m &lt;- bestorder #model order to fit stationary &lt;- F # use a stationary distribution of mixtures # different initial condition types when family == &quot;norm&quot; ic &lt;- &quot;same.sd&quot;#c(&quot;same.sd&quot;,&quot;same.both&quot;,&quot;both.diff&quot;) fd.name &lt;- ifelse(family == &quot;norm&quot;, &quot;normal&quot;, family) Pi &lt;- Pi_init(m) # T.P.M. delta &lt;- delta_init(m) pars &lt;- get.named.parlist(x, m, fd.name, lower=.0, ic)#,start=list(shape1=2,shape2=2)) # set up the model hmm &lt;- dthmm(x, Pi=Pi, delta=delta, family, pars, nonstat=!stationary, discrete = discrete) sink(&quot;p.7.best.hmm&quot;) hmm &lt;- BaumWelch(hmm, bwcontrol(maxiter = 1000, tol = 1e-08)) sink() # end hidding output decoding &lt;- Viterbi(hmm) The model summary is also attached together with the state probabilities and state sequence plots. print(summary(hmm)) ## $delta ## [1] 1.0000e+00 3.5938e-72 ## ## $Pi ## [,1] [,2] ## [1,] 0.7377182 0.2622818 ## [2,] 0.3051391 0.6948609 ## ## $nonstat ## [1] TRUE ## ## $distn ## [1] &quot;gamma&quot; ## ## $pm ## $pm$rate ## [1] 12.729694 7.226309 ## ## $pm$shape ## [1] 19.253742 7.583725 ## ## ## $discrete ## [1] FALSE ## ## $n ## [1] 111 cat(&#39;Model order:&#39;,m,&#39;\\n&#39;) ## Model order: 2 p &lt;- ggplot_stationary_hmm(hmm,.5) print(p) state.1=ifelse(decoding==2,mean(x)-0.25,NA) state.2=ifelse(decoding==1,mean(x)+0.25,NA) plot(x,type=&quot;l&quot;,ylab=&quot;&quot;, main=&quot;Modified Spring flows and states (supperposed)&quot;) points(state.2,col=&#39;blue&#39;) points(state.1,col=&#39;red&#39;) legend(&quot;topright&quot;,legend=c(&quot;state 1&quot;,&quot;state 2&quot;),col=c(&quot;red&quot;,&quot;blue&quot;),pch=1) 7.2 Generate 250 simulations each of same length as the historical data This involves generating the state sequence from the transition probability matrix and resampling flows from the corresponding component distribution. # simulate from the transition probability N = length(x) nsim = 250 nprob = length(decoding[decoding == 1])/N delta1=c(nprob,1-nprob) #stationary probability zsim = mchain(NULL,hmm$Pi,delta=delta1) spring.sim = matrix(0,nrow=nsim,ncol=N) # Points where May PDF is evaluated xeval=seq(min(x)-0.25*sd(x), max(x)+0.25*sd(x),length=100) sim.pdf=matrix(0,nrow=nsim,ncol=100) # Array to store May simulated PDF spring.stat=matrix(NA,ncol = 6,nrow = nsim) # year statistics colnames(spring.stat) = c(&quot;mean&quot;,&quot;stdev&quot;,&quot;min&quot;,&quot;max&quot;,&quot;skew&quot;,&quot;cor&quot;) for(isim in 1:nsim){ zsim = simulate(zsim,nsim=N) ## now simulate the flows from the corresponding PDF flowsim = c() for(i in 1:N){ if(zsim$mc[i] == 1)xx=rgamma(1,shape=hmm$pm$shape[1], scale=1/hmm$pm$rate[1]) if(zsim$mc[i] == 2)xx=rgamma(1,shape=hmm$pm$shape[2], scale=1/hmm$pm$rate[2]) flowsim=c(flowsim,xx) } spring.sim[isim,]=flowsim sim.pdf[isim,]=sm.density(flowsim,eval.points=xeval, display=&quot;none&quot;)$estimate # fill statistics spring.stat[isim,&quot;mean&quot;]=mean(flowsim) spring.stat[isim,&quot;max&quot;]=max(flowsim) spring.stat[isim,&quot;min&quot;]=min(flowsim) spring.stat[isim,&quot;stdev&quot;]=sd(flowsim) spring.stat[isim,&quot;skew&quot;]=skewness(flowsim) spring.stat[isim,&quot;cor&quot;]=cor(flowsim[-N],flowsim[2:N]) } The statistics from the 250 simulations and the simulated PDF are plotted with the original data. # Compute statistics from the historical data. obs=1:6 obs[1]=mean(x) obs[2]=sd(x) obs[3]=min(x) obs[4]=max(x) obs[5]=skewness(x) obs[6]=cor(x[-N],x[2:N]) # bind the stats of the historic data at the top.. spring.stat=rbind(obs,spring.stat) # function to plot boxplots with the structure: hist. in first row plot.bp = function(matrix,name){ xmeans=as.matrix(matrix) n=length(xmeans[,1]) xmeans1=as.matrix(xmeans[2:n,]) #the first row is the original data xs=1:12 zz=boxplot(split(xmeans1,col(xmeans1)), plot=F, cex=1.0) zz$names=rep(&quot;&quot;,length(zz$names)) z1=bxp(zz,ylim=range(xmeans),xlab=&quot;&quot;,ylab=&quot;&quot;,cex=1.00) points(z1,xmeans[1,],pch=16, col=&quot;red&quot;) lines(z1,xmeans[1,],pch=16, col=&quot;gray&quot;) title(main=name) } par(mfrow=c(2,3)) plot.bp(spring.stat[,&quot;mean&quot;],&quot;Mean&quot;) plot.bp(spring.stat[,&quot;stdev&quot;],&quot;Standard Deviation&quot;) plot.bp(spring.stat[,&quot;min&quot;],&quot;Min&quot;) plot.bp(spring.stat[,&quot;max&quot;],&quot;Max&quot;) plot.bp(spring.stat[,&quot;skew&quot;],&quot;Skews&quot;) plot.bp(spring.stat[,&quot;cor&quot;],&quot;Lag-1 correlation&quot;) xdensityorig = x %&gt;% sm.density(.,eval.points=xeval,display=&quot;none&quot;) %&gt;% .$estimate plot.pdf = function(eval,histPDF,simPDF){ xeval = eval plot(xeval,histPDF,pch=&quot;.&quot;,col=&quot;red&quot;,ylim=range(simPDF,histPDF), xlab=&quot;&quot;,ylab = &quot;&quot;) for(i in 1:nsim)lines(xeval,simPDF[i,],col=&#39;lightgrey&#39;,lty=3) lines(xeval,histPDF,lwd=3,col=&quot;red&quot;) title(main=&quot;Historical vs. simulated PDF&quot;) } plot.pdf(xeval,xdensityorig,sim.pdf) 7.3 Fit a best GLM to the state sequence Use the predictor as a function of preceeding winter season climate indices and the state from the previous year. enso=read.table(&quot;C:/Users/alexb/Google Drive/CVEN 6833 ADAT/zz Homeworks/HW3/enso.txt&quot;) amo=read.table(&quot;C:/Users/alexb/Google Drive/CVEN 6833 ADAT/zz Homeworks/HW3/amo.txt&quot;) pdo=read.table(&quot;C:/Users/alexb/Google Drive/CVEN 6833 ADAT/zz Homeworks/HW3/pdo.txt&quot;) X = data.frame(decoding[-N]-1,enso[-N,],amo[-N,],pdo[-N,]) colnames(X) = c(&quot;ts1&quot;,&quot;enso&quot;,&quot;amo&quot;,&quot;pdo&quot;) Y = decoding[-1]-1 links = c(&quot;logit&quot;, &quot;probit&quot;, &quot;cauchit&quot;,&quot;log&quot;,&quot;cloglog&quot;) # potential links comb=leaps(X,Y, nbest=40,method=&quot;adjr2&quot;)$which # all combinations of cov. aic &lt;- matrix(1e6,ncol=length(links),nrow = length(comb[,1])) colnames(aic) = links[1:length(links)] for(k in 1:length(comb[,1])){ # try every link f. with every comb. xx = X[,comb[k,]] %&gt;% as.data.frame(.) for(i in 1:length(links)){ zz=try(glm(Y ~ ., data=xx, family = binomial(link=links[i]), maxit=500),silent=TRUE) if(class(zz)[1]!=&quot;try-error&quot;)aic[k,i]=zz$aic[1] } } head(aic) ## logit probit cauchit log cloglog ## [1,] 97.74487 97.74487 97.74487 9.774487e+01 97.74487 ## [2,] 126.88566 126.47855 128.77622 1.000000e+06 128.60526 ## [3,] 148.52354 148.52630 148.52114 1.485643e+02 148.53909 ## [4,] 148.87507 148.86830 148.91355 1.489096e+02 148.89203 ## [5,] 92.64201 92.79124 91.54990 1.000000e+06 93.72228 ## [6,] 99.22276 99.27680 98.72933 1.000000e+06 98.71944 index = which(aic == min(aic), arr.ind = TRUE) # select min. AIC print( sprintf(paste(&quot;Choosing the GLM which minimizes AIC for binomial&quot;, &quot;family: %s link function and %s covariates&quot;), links[index[,&quot;col&quot;]], paste(colnames(X)[comb[index[,&quot;row&quot;],]],collapse = &#39;, &#39;))) ## [1] &quot;Choosing the GLM which minimizes AIC for binomial family: cauchit link function and ts1, amo covariates&quot; state.glm = glm(Y ~ ., data=X[,comb[index[,&quot;row&quot;],]], family = binomial(link=links[index[,&quot;col&quot;]])) summary(state.glm) # Model selected ## ## Call: ## glm(formula = Y ~ ., family = binomial(link = links[index[, &quot;col&quot;]]), ## data = X[, comb[index[, &quot;row&quot;], ]]) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2583 -0.4897 -0.3741 0.5019 2.2797 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.8665 0.9469 -3.027 0.00247 ** ## ts1 3.9808 1.2182 3.268 0.00108 ** ## amo 6.1710 2.5808 2.391 0.01680 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 145.29 on 109 degrees of freedom ## Residual deviance: 85.55 on 107 degrees of freedom ## AIC: 91.55 ## ## Number of Fisher Scoring iterations: 6 To evaluate if the difference of the variance of the model is significantly better we will perform an ANOVA (via Likelihood Ratio Test) of the best model vs. the most complex model (4 covariates) with the lowest AIC. glm.complex = glm(Y ~ ., data=X, family = binomial(link=&quot;cauchit&quot;), maxit=500) anova(glm.complex,state.glm, test = &quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: Y ~ ts1 + enso + amo + pdo ## Model 2: Y ~ ts1 + amo ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 105 84.252 ## 2 107 85.550 -2 -1.2984 0.5225 The variance is not significantly better and we accept H0 hypothesis. We retain the most complex model. state.glm=glm.complex 7.4 For each year based on the predictors, obtain the probabilities of the states We have used a binomial glm where state 1 = 0 and state 2 = 1. Therefore, the fitted values (ranging from 0 to 1) represent the probability of of being on state 2. plot(state.glm$fitted.values) 7.5 Using these state probabilities, simulate flow from the corresponding state PDFs state.p = 1:N # init. state probabilities state.p[1] = sum(round(state.glm$fitted.values))/length(state.glm$fitted.values) # 1906 state.p[2:N] = state.glm$fitted.values # 1907 - 2016 state.sim = 1:N # init. simulated state flow.sim.glm = matrix(0,nrow=nsim,ncol=N) sim.pdf.glm = matrix(0,nrow=nsim,ncol=100) # Array to store May simulated PDF spring.stat.glm=matrix(NA,ncol = 6,nrow = nsim) # statistics colnames(spring.stat.glm) = c(&quot;mean&quot;,&quot;stdev&quot;,&quot;min&quot;,&quot;max&quot;,&quot;skew&quot;,&quot;cor&quot;) # simulation for(isim in 1:nsim){ for(j in 1:N){ state.sim[j]=ifelse(runif(1)&lt;state.p[j],2,1) if(state.sim[j]==1){ flow.sim.glm[isim,j]=rgamma(1,shape = hmm$pm$shape[1],hmm$pm$rate[1]) }else{ flow.sim.glm[isim,j]=rgamma(1,shape = hmm$pm$shape[2],hmm$pm$rate[2]) } } sim.pdf.glm[isim,]=sm.density(flow.sim.glm[isim,], eval.points=xeval,display=&quot;none&quot;)$estimate # fill statistics spring.stat.glm[isim,&quot;mean&quot;]=mean(flow.sim.glm[isim,]) spring.stat.glm[isim,&quot;max&quot;]=max(flow.sim.glm[isim,]) spring.stat.glm[isim,&quot;min&quot;]=min(flow.sim.glm[isim,]) spring.stat.glm[isim,&quot;stdev&quot;]=sd(flow.sim.glm[isim,]) spring.stat.glm[isim,&quot;skew&quot;]=skewness(flow.sim.glm[isim,]) spring.stat.glm[isim,&quot;cor&quot;]=cor(flow.sim.glm[isim,-N],flow.sim.glm[isim,2:N]) } # bind the stats of the historic data at the top.. spring.stat.glm=rbind(obs,spring.stat.glm) par(mfrow=c(2,3)) plot.bp(spring.stat.glm[,&quot;mean&quot;],&quot;Mean&quot;) plot.bp(spring.stat.glm[,&quot;stdev&quot;],&quot;Standard Deviation&quot;) plot.bp(spring.stat.glm[,&quot;min&quot;],&quot;Min&quot;) plot.bp(spring.stat.glm[,&quot;max&quot;],&quot;Max&quot;) plot.bp(spring.stat.glm[,&quot;skew&quot;],&quot;Skews&quot;) plot.bp(spring.stat.glm[,&quot;cor&quot;],&quot;Lag-1 correlation&quot;) plot.pdf(xeval,xdensityorig,sim.pdf.glm) 7.6 Compute the median and compare with the observed flow. Compute RPSS at the terciles. The median values are compared with the observed values in a scatter plot. The correlation is also calculated. median.sim = 1:N for(i in 1:N) median.sim[i] = median(flow.sim.glm[,i]) # par(mfrow=c(1,2)) plot(x,type=&quot;l&quot;) lines(median.sim,col = &quot;red&quot;) plot(x,median.sim,xlab=&quot;Observed flows&quot;,ylab = &quot;Simulated median flows&quot;) The time series plot shows persistance on the simulated median values, with two clear states. The calculated correlation between observation and simulated median values is: cor(x,median.sim) ## [1] 0.4417929 The Ranked Probability Skill Score at the terciles is: obs = quantile(x,c(0.333,0.6667)) # flow at given terciles pred.prob = matrix(NA,ncol = nsim, nrow = 2) # Sim. Prob. at observation points baseline = c(0.333,0.6667) # Terciles for(i in 1:nsim){ aux.CDF=ecdf(flow.sim.glm[i,]) # ecdf for each simulation pred.prob[,i] = aux.CDF(obs) # cum. prob. of given flow acc. to ecdf } rps(obs,pred.prob,baseline = baseline)$rpss # RPSS ## [1] -0.08105927 The negative value indicates that the flowstream forecast at the terciles is not improved when using climate-related variables (real probabilities vs. binomial regression prob.). 7.7 Make blind predictions from 2001 and on Evaluate the performance by computing the skills as above. xx = X[95:110,] # covariates 2000 - 2010 for glm blind.flow = matrix(NA,nrow = nsim, ncol = 16) #init sim flows for(isim in 1:nsim){ s = xx[1,&quot;ts1&quot;] + 1 # initial state for the loop for(i in 1:16){ xx[i,&quot;ts1&quot;] = s-1 # replace hmm step seq. with simulated step sequence y = predict(state.glm,newdata = xx[i,],type = &quot;response&quot;) # prob of state 2 s = ifelse(runif(1)&lt;y,2,1) # state f = rgamma(1,shape=hmm$pm$shape[s],scale=1/hmm$pm$rate[s])# streamflow blind.flow[isim,i] = f # assign to time series } } boxplot(blind.flow,names = seq(1:16)+2000) title(&quot;Forecasted vs. real streamflows&quot;) lines(x[96:111],col=&quot;red&quot;) points(x[96:111],col=&quot;red&quot;) 7.7.1 Correlation of median streamflows blind.median = 1:16 for(i in 1:16){ blind.median[i] = median(blind.flow[,i]) } plot(x[96:111],blind.median) cor(x[96:111],blind.median) ## [1] 0.05956087 The correlation is significantly lower than the one obtained for the historical time series. 7.7.2 RPSS obs = quantile(x[101:111],c(0.333,0.6667)) # flow at given terciles pred.prob = matrix(NA,ncol = nsim, nrow = 2) # Sim. Prob. at observation points baseline = c(0.333,0.6667) # Terciles for(i in 1:nsim){ aux.CDF=ecdf(blind.flow[i,]) # ecdf for each simulation pred.prob[,i] = aux.CDF(obs) # cum. prob. of given flow acc. to ecdf } rps(obs,pred.prob,baseline = baseline)$rpss # RPSS ## [1] -0.3043637 The score is even more negative, showing the lack of improvement accuracy when forecasting. 7.8 Functions from lib.R # source functions # takend from &quot;http://civil.colorado.edu/~balajir/CVEN6833/R-sessions/session3/files-4HW3/lib.R&quot; Pi_init &lt;- function(n,type=&#39;uniform&#39;){ matrix(rep(1/n,n^2),n)} delta_init &lt;- function(n, type=&#39;uniform&#39;){ d &lt;- rnorm(n)^2 d/sum(d)} ntile.ts &lt;- function(x, n, limit.type = &#39;prob&#39;, tie = 1, altobs = NULL ){ # returns an integer vector corresponding to n states broken by equal # probability or equal distance # limit &lt;- if(limit.type == &#39;prob&#39;) quantile(x,seq(0,1,1/n)) else if(limit.type == &#39;equal&#39;) seq(min(x),max(x),by=diff(range(x))/n) if(!is.null(altobs)) limit &lt;- quantile(altobs,seq(0,1,1/n)) b &lt;- integer(length(x)) for(i in 1:(n+1)){ filter &lt;- if(tie == 1) x &gt;= limit[i] &amp; x &lt;= limit[i+1] else x &gt; limit[i] &amp; x &lt;= limit[i+1] #only need to set the 1&#39;s because b is already 0&#39;s b[filter] &lt;- as.integer(i-1) } if(class(x) == &#39;ts&#39;) return(ts(b,start=start(x),end=end(x))) else return(b) } get.named.parlist &lt;- function(x,m,dist,ic,...){ require(MASS) fit &lt;- fitdistr(x,dist,...) np &lt;- length(fit$estimate) pars &lt;- vector(&#39;list&#39;,np) names(pars) &lt;- names(fit$estimate) init &lt;- lapply(fit$estimate,max) names(init) &lt;- names(fit$estimate) for(j in 1:m){ #print(j) #browser() #browser() this.fit &lt;- fitdistr(x[ntile.ts(x,m) == (j-1)],dist,init,...) #for(k in 1:np) # pars[[k]][j] &lt;- this.fit$estimate[k] for(k in 1:np) pars[[k]][j] &lt;- fit$estimate[k] if(dist == &#39;normal&#39;){ if(ic == &#39;same.both&#39;){ pars[[k]][j] &lt;- mean(x) pars[[k]][j] &lt;- sd(x) } else if( ic == &#39;same.sd&#39;){ pars[[k]][j] &lt;- mean(x[ntile.ts(x,m) == (j-1)]) pars[[k]][j] &lt;- sd(x) }else{ pars[[k]][j] &lt;- mean(x[ntile.ts(x,m) == (j-1)]) pars[[k]][j] &lt;- sd(x[ntile.ts(x,m) == (j-1)]) } } } pars } AIC.dthmm &lt;- function(x){ ## Return the Akaieke Information criterion value for a fitted discrete ## time hidden markov model from the HiddenMarkov package # Model order m &lt;- length(x$delta) # Log Liklihood value LL &lt;- x$LL # number of parameters p &lt;- m+m^2 # AIC -2*LL + 2*p } ggplot_stationary_hmm &lt;- function(x,binwidth=NULL,res=1000,cols=NULL,...){ m &lt;- length(x$delta) dens &lt;- matrix(0,nrow=m+1,ncol=res) r &lt;- extendrange(x$x,f=.05) xrange &lt;- seq(r[1],r[2],len=res) delta &lt;- statdist(x$Pi) if(is.null(binwidth)) binwidth &lt;- diff(range(x$x))/8 for(i in 1:m){ if(x$distn == &#39;gamma&#39;){ dens[i,] &lt;- delta[i]*dgamma(xrange,shape=x$pm$shape[i],rate=x$pm$rate[i]) }else if(x$distn == &#39;norm&#39;){ dens[i,] &lt;- delta[i]*dnorm(xrange,mean=x$pm$mean[i],sd=x$pm$sd[i]) }else{ stop(&#39;Distribution not supported&#39;) } dens[m+1,] &lt;- dens[m+1,] + dens[i,] } p &lt;- ggplot()+ geom_histogram(data=data.frame(x=as.vector(x$x)),aes(x=x,y=..density..), binwidth=binwidth,fill=&#39;white&#39;,color=&#39;black&#39;)+ theme_bw() dt &lt;- data.table(x=numeric(0),y=numeric(0), state=integer(0)) for(i in 1:m) dt &lt;- rbind(dt, data.table(x=xrange,y=dens[i,], state=i)) dt$state &lt;- factor(dt$state) p &lt;- p + geom_line(data=dt,aes(x=x,y=y,color=state)) + geom_line(data=data.frame(x=xrange,y=dens[m+1,]),aes(x=x,y=y),color=&#39;black&#39;,size=1) + scale_color_tableau() + scale_x_continuous(limits=r) p } statdist &lt;- function(tpm){ m &lt;- nrow(tpm) ones &lt;- rbind(rep(1,m)) I &lt;- diag(rep(1,m)) U &lt;- matrix(rep(1,m^2),m) as.vector(ones %*% solve(I - tpm + U)) } "],
["singular-spectrum-analysis-diagnostics-forecasting.html", "Exercise 8 Singular Spectrum Analysis – Diagnostics &amp; Forecasting 8.1 SSA on time frame window 8.2 Prediction: Use AR models to fit RPC and predict 2001 - 2016 streamflows", " Exercise 8 Singular Spectrum Analysis – Diagnostics &amp; Forecasting For the spring season flow (Apr-Jun average) at Lees Ferry on the Colorado River perform SSA and make predictions from it. The steps are as follows: ** Diagnostic ** Select a window size of about 10-20 years (feel free to experiment with the window size); create the Toeplitz matrix and perform SSA. Plot the Eigen spectrum and identify the dominant modes. Reconstruct the dominant modes (i.e. Reconstructed Components - RCs) and plot them. Infer from them the dominant periodicities. Sum the leading modes and plot them along with the original time series. This will show the ‘filtering’ capability of SSA. Feel free to play with the number of RCs. Plot the dominant modes and show their corresponding wavelet spectra ** Prediction ** Apply the SSA to data for the pre-2001 Make a prediction for the 2001 using AR models to the leading modes; and repeat for each year through 2016. Plot the observed and predicted values; compute the median correlation. 8.1 SSA on time frame window set.seed(1) # allow repetition of results # libraries libr=c(&quot;magrittr&quot;,&quot;MASS&quot;,&quot;sm.density&quot;) options(warn=-999) suppressPackageStartupMessages(lapply(libr, require, character.only = TRUE)) # Load flow data mflow = read.table( &quot;http://civil.colorado.edu/~balajir/CVEN6833/HWs/HW-3-2018/LeesFerry-monflows-1906-2016.txt&quot;) lf = mflow[,2:13] %&gt;% `rownames&lt;-`(mflow[,1]) %&gt;% # Apr - Jun {rowMeans(.[,4:6])} %&gt;% {.*0.0004690502*0.001} # convert to 10^3 cms # lfann = apply(mflow[,2:13],1,sum) %&gt;% {.*0.0004690502*0.001} M = 20 ssaout = ssab(lf,M) par(mfrow=c(1,2)) plot(ssaout$lambdas) plot(cumsum(ssaout$lambdas)) sum(ssaout$lambdas[1:5]) ## [1] 0.4127211 We choose the first 5 RPC as they represent more than 40% of the variance. K=5 par(mfrow=c(3,2)) for(i in 1:K){ plot(ssaout$Rpc[,i],type = &quot;l&quot;,main = paste(sprintf(&quot;RPC %s&quot;,i))) } The resulting time series from the reconstructed dominant modes is plotted with the historical data. Recon = apply(ssaout$Rpc[,1:K],1,sum) plot(mflow[,1],lf,col=&quot;red&quot;,type=&quot;l&quot;,xlab = &quot;&quot;, ylab = &quot;&quot;,ylim = c(0,max(lf))) lines(mflow[,1],Recon,col=&quot;blue&quot;) legend(x=&quot;bottomleft&quot;,legend = c(&quot;Historic values&quot;, &quot;Reconstructed dominant modes&quot;),col=c(&quot;red&quot;,&quot;blue&quot;),lty = 1,bty=&quot;n&quot;) The sum of all RPC gives the original time series. Recon.tot = apply(ssaout$Rpc[,1:20],1,sum) plot(mflow[,1],lf,col=&quot;red&quot;,type=&quot;l&quot;,xlab = &quot;&quot;, ylab = &quot;&quot;,ylim = c(0,max(lf))) points(mflow[,1],Recon.tot,col=&quot;blue&quot;) legend(x=&quot;bottomleft&quot;,legend = c(&quot;Historic values&quot;, &quot;Reconstructed dominant modes&quot;),col=c(&quot;red&quot;,&quot;blue&quot;),lty = 1,bty=&quot;n&quot;) 8.2 Prediction: Use AR models to fit RPC and predict 2001 - 2016 streamflows 8.2.1 Perform SSA until 2001 M = 15 # window reduced ssaout = ssab(lf[1:95],M) # 1906 - 2000 par(mfrow=c(1,2)) plot(ssaout$lambdas) plot(cumsum(ssaout$lambdas)) sum(ssaout$lambdas[1:5]) ## [1] 0.4784262 We choose the first 5 RPC as they represent more than 40% of the variance. K=5 par(mfrow=c(3,2)) for(i in 1:K){ plot(ssaout$Rpc[,i],type = &quot;l&quot;,main = paste(sprintf(&quot;RPC %s&quot;,i))) } The resulting time series from the reconstructed dominant modes is plotted with the historical data. Recon = apply(ssaout$Rpc[,1:K],1,sum) plot(mflow[1:95,1],lf[1:95],col=&quot;red&quot;,type=&quot;l&quot;,xlab = &quot;&quot;, ylab = &quot;&quot;,ylim = c(0,max(lf))) lines(mflow[1:95,1],Recon,col=&quot;blue&quot;) legend(x=&quot;bottomleft&quot;,legend = c(&quot;Historic values&quot;, &quot;Reconstructed dominant modes&quot;),col=c(&quot;red&quot;,&quot;blue&quot;),lty = 1,bty=&quot;n&quot;) 8.2.2 AR model for dominant modes # Fit best AR to each dominant RPC rc.ar = as.list(1:K) for(i in 1:K) rc.ar[[i]] = ar(ssaout$Rpc[,i]) # Fit a Normal to RPC 6 to 15 resid = apply(ssaout$Rpc[,(K+1):M],1,sum) noise = fitdistr(resid,densfun = &quot;normal&quot;)$estimate hist(resid, probability = TRUE) sm.density(rnorm(1000,noise[1],noise[2]),add = TRUE) # Simulate 2001 to 2016 nsim = 250 pred.ahead = se.ahead = array(NA, c(K, 16, nsim)) sim = matrix(NA,nrow = nsim,ncol = 16) for(isim in 1:nsim){ for(i in 1:K){ xp = predict(rc.ar[[i]],n.ahead = 16) pred.ahead[i,,isim] = xp$pred se.ahead[i,,isim] = xp$se } # Add all RPC and add noise (normally distributed) sim[isim,] = colSums(pred.ahead[,,isim]) + rnorm(16,noise[1],noise[2]) } # plot(sim,type = &quot;l&quot;,ylim = c(0,max(sim,lf[96:111]))) # lines(lf[96:111],col = &quot;red&quot;) # legend(x=&quot;topleft&quot;,legend = c(&quot;Historic values&quot;, # &quot;Simulated values&quot;),col=c(&quot;red&quot;,&quot;black&quot;),lty = 1,bty=&quot;n&quot;) The forecast is presented as boxplots. The correlation is calculated using the median values. # Median correlation boxplot(sim,names = seq(1:16)+2000) title(&quot;Forecasted vs. real streamflows&quot;) lines(lf[96:111],col=&quot;red&quot;) points(lf[96:111],col=&quot;red&quot;) sim.median = apply(t(sim),1,median) # Get median value of each step cor(sim.median, lf[96:111]) # Correlation ## [1] 0.2562116 ssab = function(yy,M) { #### SSA N = length(yy) m1 = M-1 Np = N-M+1 Np1 = Np+1 ## create the lagged matrix topl = matrix(0,Np,M) for(i in 1:M){ i1=i-1+1 i2 = Np+i-1 topl[,i]=yy[i1:i2] } #zz=scale(topl) zs=var(topl) zsvd = svd(zs) #Eigen Values.. - fraction variance lambdas=(zsvd$d/sum(zsvd$d)) ### PCs At = topl %*% zsvd$u #### Reconstructed Components RCs Rpc = matrix(0,N,M) for(ipc in 1:M){ ## t = 1,M-1 mt = 1/t; lt = 1, ut =t lt=1 m1 = M-1 for(i in 1:m1){ i1 = i-lt+1 i2 = i-i+1 Rpc[i,ipc] = sum(At[i1:i2,ipc]*zsvd$u[lt:i,ipc])/i } ### t = M, N&#39;, mt = 1/M, lt = 1, ut = M lt=1 ut=M for(i in M:Np){ i1 = i-lt+1 i2 = i-M+1 Rpc[i,ipc] = sum(At[i1:i2,ipc]*zsvd$u[lt:ut,ipc])/M } ### t = N&#39;+1, N, mt = 1/(N-t+1), lt = t-N+M, ut = M lt=1 ut=M for(i in Np1:N){ lt = i-N+M i1 = i-lt+1 i2 = i-M+1 mt = (N-i+1) Rpc[i,ipc] = sum(At[i1:i2,ipc]*zsvd$u[lt:ut,ipc])/mt } } out=c() out$lambdas = lambdas out$eof = zsvd$u out$At = At out$Rpc = Rpc as.list(out) } "]
]
